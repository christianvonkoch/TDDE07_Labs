---
title: "Exams"
author: "Christian von Koch"
date: '2020-05-31'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#2017-05-30

## Assignment 1

```{r eval=FALSE}
## a) Plot the posterior distribution of theta

riceData <- c(1.556, 1.861, 3.135, 1.311, 1.877, 0.622, 3.219, 0.768, 2.358, 2.056)

# Random number generator for the Rice distribution
rRice <-function(n = 1, theta = 1, psi = 1){
  x <- rnorm(n = n, mean = 0, sd = sqrt(psi))
  y <- rnorm(n = n, mean = theta, sd = sqrt(psi))
  return(sqrt(x^2+y^2))
}

# Function for calculating the log posterior distrib with theta prior set to 1
logPosterior = function(data, theta, psi) {
  bessel_factor=1
  for (i in data) {
    bessel_factor=bessel_factor*besselI(i*theta/psi, nu=0)
  }
  post=-log(psi)-1/(2*psi)*sum(data^2+theta^2)+log(bessel_factor)
  return(post+0) # If prior is assumed to be constant we set the prior to 1 which in log scale yields 0
}

gridWidth=0.01
theta_grid=seq(0,3,gridWidth)
posterior_distrib_log=sapply(theta_grid, logPosterior, data=riceData, psi=1)
posterior_distrib_norm=1/gridWidth*exp(posterior_distrib_log)/sum(exp(posterior_distrib_log))
sum(posterior_distrib_norm)
plot(theta_grid, posterior_distrib_norm, xlab=expression(theta), ylab="Density", main="Posterior density of theta",
     type="l", lwd=2)

## b) Use numerical optimization to obtain a normal approx. of the posterior distrib of theta. Overlay curve 
## from a) with the approximated normal distribution

# Defining initial values to be passed on to the optimizer
set.seed(12345)
initVal = rnorm(1, mean=0, sd=1)

# Finding the optimized betavector
optimResult = optim(initVal, logPosterior, data=riceData, psi=1, method=c("L-BFGS-B"),
                    control=list(fnscale=-1), lower=0, hessian=TRUE)

# Defining the values of interest
postMode = optimResult$par
postCov = as.numeric(-solve(optimResult$hessian))
print("The posterior mode is:")
print(postMode)
print("The approximated standard deviation is:")
print(postCov)
lines(theta_grid, dnorm(theta_grid, mean=postMode, sd=sqrt(postCov)), col="red", lwd=2)
legend(x = 1.8, y = 1, legend = c("True posterior", "Approximate posterior"), 
       col = c("black","red"), lty = c(1,1), lwd = c(2,2), cex = 0.8)

## Answer: Not perfect approx but fairly good. 

## c) Simulate distrib for new observation using normal approx in b)

nDraws=5000
set.seed(12345)
theta=rnorm(nDraws, mean=postMode, sd=sqrt(postCov))
pred_distrib=c()
for (i in theta) {
  pred_distrib=c(pred_distrib, rRice(theta=i))
}

hist(pred_distrib, breaks=100, xlab="Index", main="Predictive density of new obs")
```

##Assignment 2

```{r eval=FALSE}
## a) Model posterior data with prior Gamma and likelihood Poisson, plot the posterior

# We know that posterior mapping with gamma prior and poisson likelihood is gamma distributed

sumBids=sum(bids)
n=length(bids)
alpha=1
beta=1
posterior_theta=dgamma(seq(3,4,0.001), alpha+sumBids, beta+n)
plot(seq(3,4,0.001), posterior_theta, type="l", lwd=2)

# b) Investigate through graphical methods if Poisson model describes data well

xGrid=seq(min(bids), max(bids))
data_norm=bidsCounts/sum(bidsCounts)
nDraws=5000
thetaDraws=rgamma(nDraws, alpha+sumBids, beta+n)
poissonDensity=rep(0, length(xGrid))
for (i in thetaDraws) {
  poissonDensity=poissonDensity+dpois(xGrid, lambda=i)
}

avgPoissonDensity=poissonDensity/nDraws
plot(xGrid, data_norm, xlab="No. of bids", ylab="Density", main="Fitted models", type="o", cex=0.8,
     ylim=c(0,0.25), lwd=2)
lines(xGrid, avgPoissonDensity, col="red", lwd=2, type="o")
legend(x=7, y=0.2, col=c("black", "red"), legend=c("Data", "Poisson mean density"), lty=c(1,1), 
       lwd=c(2,2), pch=c("o", "o"))

## Terrible fit which the plot shows

## c) Use GibbsMixPois.R. Esimate the mixture of Poissons both with K=2 and K=3. nIter=5000.

GibbsMixPois <- function(x, nComp, alpha, alphaGamma, betaGamma, xGrid, nIter){
  
  # Gibbs sampling for a mixture of Poissons
  # Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
  #
  # INPUTS:
  #   x - vector with data observations (counts)
  #   nComp - Number of mixture components to be fitted
  #   alpha - The prior on the mixture component weights is w ~ Dir(alpha, alpha,..., alpha) 
  #   alphaGamma and betaGamma - 
  #              The prior on the mean (theta) of the Poisson mixture components is 
  #              theta ~ Gamma(alphaGamma, betaGamma) [rate parametrization of the Gamma dist]
  #   xGrid - the grid of data values over which the mixture is evaluated and plotted
  #   nIter - Number of Gibbs iterations
  #
  # OUTPUTS:
  #   results$wSample     - Gibbs sample of mixture component weights. nIter-by-nComp matrix
  #   results$thetaSample - Gibbs sample of mixture component means.   nIter-by-nComp matrix
  #   results$mixDensMean - Posterior mean of the estimated mixture density over xGrid.
  
  
  ####### Defining a function that simulates from a Dirichlet distribution
  rDirichlet <- function(param){
    nCat <- length(param)
    thetaDraws <- matrix(NA,nCat,1)
    for (j in 1:nCat){
      thetaDraws[j] <- rgamma(1,param[j],1)
    }
    thetaDraws = thetaDraws/sum(thetaDraws) # Diving every column of ThetaDraws by the sum of the elements in that column.
    return(thetaDraws)
  }
  
  # Simple function that converts between two different representations of the mixture allocation
  S2alloc <- function(S){
    n <- dim(S)[1]
    alloc <- rep(0,n)
    for (i in 1:n){
      alloc[i] <- which(S[i,] == 1)
    }
    return(alloc)
  }
  
  # Initial values for the Gibbs sampling
  nObs <- length(x)
  S <- t(rmultinom(nObs, size = 1 , prob = rep(1/nComp,nComp))) # nObs-by-nComp matrix with component allocations.
  theta <- rep(mean(x), nComp) # Each component is initialized at the mean of the data
  
  # Setting up the grid where the mixture density is evaluated.
  mixDensMean <- rep(0,length(xGrid))
  effIterCount <- 0
  
  # Setting up matrices to store the draws
  wSample <- matrix(0, nIter, nComp)
  thetaSample <- matrix(0, nIter, nComp)
  probObsInComp <- rep(NA, nComp)
  
  # Setting up the priors - the same prior for all components
  alpha <- rep(alpha, nComp) 
  alphaGamma <- rep(alphaGamma, nComp) 
  betaGamma <- rep(betaGamma, nComp) 
  
  # HERE STARTS THE ACTUAL GIBBS SAMPLING
  
  for (k in 1:nIter){
    message(paste('Iteration number:',k))
    alloc <- S2alloc(S) # Function that converts between different representations of the group allocations
    nAlloc <- colSums(S)
    
    # Step 1 - Update components probabilities
    w <- rDirichlet(alpha + nAlloc)
    wSample[k,] <- w
    
    # Step 2 - Update theta's in Poisson components
    for (j in 1:nComp){
      theta[j] <- rgamma(1, shape = alphaGamma + sum(x[alloc == j]), rate = betaGamma + nAlloc[j])
    }
    thetaSample[k,] <- theta
    
    # Step 3 - Update allocation
    for (i in 1:nObs){
      for (j in 1:nComp){
        probObsInComp[j] <- w[j]*dpois(x[i], lambda = theta[j])
      }
      S[i,] <- t(rmultinom(1, size = 1 , prob = probObsInComp/sum(probObsInComp)))
    }
    
    # Computing the mixture density at the current parameters, and averaging that over draws.
    effIterCount <- effIterCount + 1
    mixDens <- rep(0,length(xGrid))
    for (j in 1:nComp){
      compDens <- dpois(xGrid, lambda = theta[j])
      mixDens <- mixDens + w[j]*compDens
    }
    mixDensMean <- ((effIterCount-1)*mixDensMean + mixDens)/effIterCount
  }
  return(results = list(wSample = wSample, thetaSample = thetaSample, mixDensMean = mixDensMean))
}

result_comp2=GibbsMixPois(bids, nComp=2, alpha=1, alphaGamma = alpha, betaGamma = beta, 
                    xGrid=xGrid, nIter=500)
result_comp3=GibbsMixPois(bids, nComp=3, alpha=1, alphaGamma = alpha, betaGamma = beta, 
                          xGrid=xGrid, nIter=500)

## c) Use graphical methods to investigate if mixture of poissons fits data well. Is K=2 enough or should we
## use K=3?

plot(xGrid, data_norm, xlab="No. of bids", ylab="Density", main="Fitted models", type="o",
     ylim=c(0,0.25), lwd=2)
lines(xGrid, result_comp2$mixDensMean, col="red", lwd=2, type="o")
lines(xGrid, result_comp3$mixDensMean, col="gray", lwd=2, type="o")
legend(x=7, y=0.2, col=c("black", "red", "gray"), 
       legend=c("Data", "Mixture density with 2 components", "Mixture density with 3 components"), 
       lty=c(1,1,1), lwd=c(2,2, 2), pch=c("o", "o", "o"), cex=1)

## Good enough with 2 components in the mixture density

```

##Assignment 3

```{r eval=FALSE}
###############################
########## Problem 3 ########## 
############################### 

# Reading the cars data from file
load("cars.RData")

library(mvtnorm)

# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
  # Direct sampling from a Gaussian linear regression with conjugate prior:
  #
  # beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
  # sigma2 ~ Inv-Chi2(v_0,sigma2_0)
  # 
  # Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
  #
  # INPUTS:
  #   y - n-by-1 vector with response data observations
  #   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
  #   mu_0 - prior mean for beta
  #   Omega_0  - prior precision matrix for beta
  #   v_0      - degrees of freedom in the prior for sigma2
  #   sigma2_0 - location ("best guess") in the prior for sigma2
  #   nIter - Number of samples from the posterior (iterations)
  #
  # OUTPUTS:
  #   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
  #   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
  
  # Compute posterior hyperparameters
  n = length(y) # Number of observations
  nCovs = dim(X)[2] # Number of covariates
  XX = t(X)%*%X
  betaHat <- solve(XX,t(X)%*%y)
  Omega_n = XX + Omega_0
  mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
  v_n = v_0 + n
  sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
  invOmega_n = solve(Omega_n)
  
  # The actual sampling
  sigma2Sample = rep(NA, nIter)
  betaSample = matrix(NA, nIter, nCovs)
  for (i in 1:nIter){
    
    # Simulate from p(sigma2 | y, X)
    sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
    sigma2Sample[i] = sigma2
    
    # Simulate from p(beta | sigma2, y, X)
    beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
    betaSample[i,] = beta_
    
  }
  return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}

## a) Linear regression problem with given dataset. Use Mattias function to derive joint posterior. 
## i) Plot marginal distributions of each param
## ii) Compute point estimates for each regression coefficient assuming loss function
## iii) Construct 95 % equal tail probability intervals for each parameter and interpret them


y=cars$mpg
x=as.matrix(cars[2:ncol(cars)])
mu_0=c(0,0,0,0)
omega_0=0.01*diag(x=4)
nu_0=1
sigma_sq_0=36
jointPostDistrib=BayesLinReg(y, x, mu_0, omega_0, nu_0, sigma_sq_0, 1000)
hist(jointPostDistrib$sigma2Sample, breaks=10, main=paste("Marginal distribution of", expression(sigma^2)),
     xlab=expression(sigma^2))
par(mfrow=c(2,2))
for(i in 1:4) {
  hist(jointPostDistrib$betaSample[,i], breaks=10, main=paste("Marginal distribution of ", expression(beta), i, 
                                                              sep=""), xlab=paste(expression(beta),i, sep=""))
}
title("Marginal distributions of the different betavalues", line=-1, outer=TRUE)
par(mfrow=c(1,1))

# Linear loss function is posterior median
median(jointPostDistrib$sigma2Sample)
median(jointPostDistrib$betaSample[,1])
median(jointPostDistrib$betaSample[,2])
median(jointPostDistrib$betaSample[,3])
median(jointPostDistrib$betaSample[,4])

# Prediction intervals for each param
quantile(jointPostDistrib$sigma2Sample, c(0.025, 0.975))
quantile(jointPostDistrib$betaSample[,1], c(0.025, 0.975))
quantile(jointPostDistrib$betaSample[,2], c(0.025, 0.975))
quantile(jointPostDistrib$betaSample[,3], c(0.025, 0.975))
quantile(jointPostDistrib$betaSample[,4], c(0.025, 0.975))

## Answer: Interpretation of the credible interval for weight [-4.759964, -1.531457]. A one unit increase of weight
## lowers the amount of miles per gallon between -4.759964 and -1.531457 with 95 % posterior probability. 

## b) Investigate if effect on mpg is different in cars with six cylinders compared to cars with 8 cylinders

hist(jointPostDistrib$betaSample[,4]-jointPostDistrib$betaSample[,3], 50)
quantile(jointPostDistrib$betaSample[,4]-jointPostDistrib$betaSample[,3], c(0.025, 0.975))

## Answer: Since 0 is present in interval we can not say that there is a difference between 8 and 6 cylinders
## with 95 % posterior probability.

## c) Compute by simulation predictive distrib for a new car 4 cylinders and weight=3.5

new_x=c(1,3.5,0,0)
pred_y=rep(0,nIter)
for (i in 1:nIter) {
  pred_y[i]=sum(new_x*jointPostDistrib$betaSample[i,])+rnorm(1,sd=sqrt(jointPostDistrib$sigma2Sample[i]))
}
hist(pred_y, breaks=40, freq=FALSE)
```

##Assignment 4

```{r eval=FALSE}
## Maximizing posterior expected utility.

post_dens = function(x) {
  return(gamma(6+x)/gamma(13+x))
}

barplot(post_dens(seq(0,10,1)), type="l")

# x6=10 seems to yield a low enough probability to be an upper bound for sum

posterior_prob=post_dens(seq(0,10))
posterior_prob=posterior_prob/sum(posterior_prob)
exp_util=c()
for (k in 0:10) {
  exp_util=c(exp_util,(2^k-3))
}

exp_post_dens=sum(posterior_prob*exp_util)
print(exp_post_dens)
```

#2017-08-16

##Assignment 1

```{r eval=FALSE}
## a) Plot posterior density of theta, with normal prior and cauchy distrib as likelihood

# Reading the data vector yVect from file
load(file = 'CauchyData.RData')
cauchydata=yVect

dCauchy <- function(x, theta = 0, gamma = 1){
  return(dens = (1/(pi*gamma))*(1/(1+((x-theta)/gamma)^2)))
}

dlognormal <- function(x, mu, sigma2){
  return(dens = (1/(sqrt(2*pi*sigma2)*x))*exp((-1/(2*sigma2))*(log(x)-mu)^2))
}

logPrior_theta = function(theta, mu, sigma_sq) {
  return(dnorm(theta, mean=mu, sd=sqrt(sigma_sq), log=TRUE))
}

logPosterior = function(data, mu, sigma_sq, theta=0, gamma=1) {
  prior=logPrior(theta, mu, sigma_sq)
  likelihood=dCauchy(data, theta, gamma)
  likelihood=sum(log(likelihood))
  return(likelihood + prior)
}

mu=0
sigma_sq=100
gamma=1
gridWidth=0.01
theta_grid=seq(0,8,gridWidth)
posterior_distrib=sapply(theta_grid, logPosterior, data=cauchydata, mu=mu, sigma_sq=sigma_sq, gamma=1)
posterior_distrib=1/gridWidth*exp(posterior_distrib)/sum(exp(posterior_distrib))
plot(theta_grid, posterior_distrib, type="l", lwd=2, main="Posterior density for theta", xlab=expression(theta),
     ylab="Density")

## b) gamma is unknown with prior lognormal. 

set.seed(12345)
initVal = c(0,0)

logJointPosterior = function(joint, data, mu, sigma_sq) {
  prior_theta=logPrior(joint[1], mu, sigma_sq)
  prior_gamma=log(dlognormal(joint[2], mu, sigma_sq))
  likelihood=dCauchy(data, joint[1], joint[2])
  likelihood=sum(log(likelihood))
  return(likelihood + prior_theta + prior_gamma)
}

# Finding the optimized theta and gamma
optimResult = optim(initVal, logJointPosterior, data=cauchydata, mu=mu, sigma_sq=sigma_sq, method=c("L-BFGS-B"),
                    control=list(fnscale=-1), lower=c(-Inf, 0.001), upper=c(Inf, Inf), hessian=TRUE)

# Defining the values of interest
postMode = optimResult$par
postCov = -solve(optimResult$hessian)
names(postMode)=c("Theta", "Gamma")
print("The posterior mode is:")
print(postMode)
print("The approximated standard deviation is:")
print(postCov)

## c) Use normal approx in 1b) to obtain marginal posterior for the 99 % percentile of the caucy distrib
## theta + gamma * tan(pi(0.99-0.5))

library(rmvnorm)
normal_approx=rmvnorm(5000, mean=postMode, sigma=postCov)
cauchy_distrib=normal_approx[,1]+normal_approx[,2]*tan(pi*(0.99-0.5))
hist(cauchy_distrib, breaks=50, main="Marginal distribution of special case of caucby", xlab="Function value")
```

##Assignment 2

```{r eval=FALSE}
# Reading the data from file
library(MASS)
BostonHousing = Boston
y = BostonHousing$medv
X = cbind(1,BostonHousing[,1:13]) # Adding a column of ones for the intercept
names(X)[1] <- "intercept"
covNames <- names(X)
y <- as.numeric(y)
X <- as.matrix(X)

library(mvtnorm)

# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
  # Direct sampling from a Gaussian linear regression with conjugate prior:
  #
  # beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
  # sigma2 ~ Inv-Chi2(v_0,sigma2_0)
  # 
  # Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
  #
  # INPUTS:
  #   y - n-by-1 vector with response data observations
  #   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
  #   mu_0 - prior mean for beta
  #   Omega_0  - prior precision matrix for beta
  #   v_0      - degrees of freedom in the prior for sigma2
  #   sigma2_0 - location ("best guess") in the prior for sigma2
  #   nIter - Number of samples from the posterior (iterations)
  #
  # OUTPUTS:
  #   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
  #   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
  
  # Compute posterior hyperparameters
  n = length(y) # Number of observations
  nCovs = dim(X)[2] # Number of covariates
  XX = t(X)%*%X
  betaHat <- solve(XX,t(X)%*%y)
  Omega_n = XX + Omega_0
  mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
  v_n = v_0 + n
  sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
  invOmega_n = solve(Omega_n)
  
  # The actual sampling
  sigma2Sample = rep(NA, nIter)
  betaSample = matrix(NA, nIter, nCovs)
  for (i in 1:nIter){
    
    # Simulate from p(sigma2 | y, X)
    sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
    sigma2Sample[i] = sigma2
    
    # Simulate from p(beta | sigma2, y, X)
    beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
    betaSample[i,] = beta_
    
  }
  return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}

mu_0=rep(0, ncol(X))
omega_0=0.01*diag(ncol(X))
v_0=1
sigma2_0=36
nIter=5000

bayes_lin_results=BayesLinReg(y, X, mu_0, omega_0, v_0, sigma2_0, nIter)
# Under quadratic loss, posterior mean is point estimate
beta_estimates=rep(0,ncol(X))
beta_credIntervals=matrix(0, ncol(X), 2)
for (i in 1:ncol(X)) {
  beta_estimates[i]=mean(bayes_lin_results$betaSample[,i])
  beta_credIntervals[i,]=quantile(bayes_lin_results$betaSample[,i], c(0.025, 0.975))
}
sigma_estimate=mean(bayes_lin_results$sigma2Sample)
sigma_credInterval=quantile(bayes_lin_results$sigma2Sample, c(0.025, 0.975))
rownames(beta_credIntervals)=covNames
beta_credIntervals[which(rownames(beta_credIntervals)=="rm"), ]

## Interpretation: for one unit increase of rooms the hosing prices will rise between 3991,475 and 5009,826 dollars
## with 95 % posterior probability. 

## b) Owner of house 381 is considering selling their house. Bought house for 10400

old_obs=as.vector(X[381,])
new_obs=old_obs
new_obs[2]=10
pred_draw=rep(0,nIter)
for (i in 1:nIter) {
  pred_draw[i]=bayes_lin_results$betaSample[i,]%*%new_obs+rnorm(1, mean=0,
                                                                sd=sqrt(bayes_lin_results$sigma2Sample[i]))
}
pred_mean=mean(pred_draw)
hist(pred_draw, breaks=50)
quantile(posterior_prices, c(0.025, 0.975))
sum(pred_draw>=30)/nIter

## c) See paper.
```

##Assignment 4

```{r eval=FALSE}
## a) Simulate 1000 draws from the posterior distrib of theta using conjugate prior for theta with mean 250
## and std = 50. Poisson likelihood. 

data=c(220,323,174,229)
alpha=25
beta=0.1
n=length(data)

logPriorGamma = function(theta, alpha, beta) {
  return(dgamma(theta, 50, beta, log=TRUE))
}

logLike = function(data, theta) {
  n=length(data)
  first=sum(data)*log(theta)
  second=theta*n
  third=0
  for (i in data) {
    for (j in 1:i) {
      third=third+log(j)
    }
  }
  return(first-second-third)
}

logPosterior = function(data, theta, alpha, beta) {
  prior=logPriorGamma(theta, alpha, beta)
  likelihood=logLike(data, theta)
  return(likelihood + prior)
}

# Conjugate prior for poisson is Gamma(alpha, beta), we know that posterior is Gamma(alpha + sum(data), beta+n)
post_draws=rgamma(1000, alpha+sum(data), beta+n)
hist(post_draws, main="Posterior distribution of theta", xlab=expression(theta))

## b) Simulate 1000 draws from the predictive distrib of next quarter's demand, X5, and plot the draws
## in histogram. 

q5=rpois(1000, post_draws)
hist(q5, breaks=50, main="Predictive distribution of quarter 5", xlab="Qty")
sum(q5<=200)/1000

## c) 


utility <- function(a,X5){
  util = rep(0,length(X5))
  util[X5<=a] = 10*X5[X5<=a]-(a-X5[X5<=a])
  util[X5>a] = 10*a-0.05*(X5[X5>a]-a)^2
  return(util)
}

mean(q5)
a=seq(136,336,1)
results = matrix(0,length(q5),length(a))
count=1
nameVec=rep(0,length(a))
for (i in a) {
  results[,count]=utility(i,q5)
  nameVec[count]=as.character(i)
  count=count+1
}
opt_vector=matrix(0,1,length(a))
for (i in 1:length(a)) {
  opt_vector[i]=mean(results[,i])
}
colnames(opt_vector)=nameVec
opt_decision=as.numeric(opt_vector[,which(opt_vector==max(opt_vector))])
names(opt_vector[,which(opt_vector==max(opt_vector))])
plot(a, opt_vector, type="l", lwd=1, col="red")
abline(v=as.numeric(names(opt_vector[,which(opt_vector==max(opt_vector))])), col="blue")
```

#2017-10-27

##Assignment 1

```{r eval=FALSE}
## a) Likelihood: Beta symmetric, prior, expon(1). Plot posterior distrib.

thetaGrid=seq(0.01, 15, length=1000)
data=yProp
lambda=1

logPriorExp = function(theta, lambda) {
  return(dexp(theta, rate=lambda, log=TRUE))
}

logPosterior = function(x, theta, lambda) {
  prior=logPriorExp(theta, lambda)
  likelihood=sum(dbeta(x, theta, theta, log=TRUE))
  return(likelihood+prior)
}

theta_post=sapply(thetaGrid, logPosterior, x=data, lambda=lambda)
theta_post_norm=1/((15-0.01)/1000)*exp(theta_post)/sum(exp(theta_post))
plot(thetaGrid, theta_post_norm, type="l", lwd=2, xlab=expression(theta), ylab="Posterior density")

# Zero to 1 loss means posterior mode is the optimal point estimator

index=which(theta_post_norm==max(theta_post_norm))
opt_theta=thetaGrid[index]
print(opt_theta)

## Optimal theta is around 4.481491

## b) Theta1 and theta2 are independent apriori. Plot joint posterior distrib

logPosteriorMult = function(theta, x, lambda) {
  theta1=theta[1]
  theta2=theta[2]
  prior1=logPriorExp(theta1, lambda)
  prior2=logPriorExp(theta2, lambda)
  likelihood=sum(dbeta(x, theta1, theta2, log=TRUE))
  return(likelihood+prior1+prior2)
}

# Defining initial values to be passed on to the optimizer
initVal = c(1,1)

# Finding the optimized betavector
optimResult = optim(initVal, logPosteriorMult, x=data, lambda=1, method=c("L-BFGS-B"),
                    control=list(fnscale=-1), lower=c(0.01,0.01), upper=c(Inf, Inf), hessian=TRUE)

# Defining the values of interest
postMode = optimResult$par
postCov = -solve(optimResult$hessian)
names(postMode)=c("Theta1", "Theta2")
rownames(postCov)=c("Theta1", "Theta2")
colnames(postCov)=c("Theta1", "Theta2")
print("The posterior mode is:")
print(postMode)
print("The approximated standard deviation is:")
print(postCov)

## c) Discuss how a Bayesian can determine if the symmetric model in 1a) or the non-symmetric model in 1b) 
## is most appropriate for this data. No need to compute anything here, just discuss.

## By calculating marginal likelihood for each model and check which has the highest. One can also calculate
## the bayes factor or the posterior model probabilities and choose the model with the highest probability.
```

##Assignment 2

```{r eval=FALSE}
## a) Use conjugate priors, standard normal and invchisq and use BayesLinReg to simulate 5000 draws from posterior
## distrib

# Reading the data from file
library(MASS)
BostonHousing = Boston
y = BostonHousing$medv
X = cbind(1,BostonHousing[,1:13]) # Adding a column of ones for the intercept
names(X)[1] <- "intercept"
covNames <- names(X)
y <- as.numeric(y)
X <- as.matrix(X)

library(mvtnorm)

# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
  # Direct sampling from a Gaussian linear regression with conjugate prior:
  #
  # beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
  # sigma2 ~ Inv-Chi2(v_0,sigma2_0)
  # 
  # Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
  #
  # INPUTS:
  #   y - n-by-1 vector with response data observations
  #   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
  #   mu_0 - prior mean for beta
  #   Omega_0  - prior precision matrix for beta
  #   v_0      - degrees of freedom in the prior for sigma2
  #   sigma2_0 - location ("best guess") in the prior for sigma2
  #   nIter - Number of samples from the posterior (iterations)
  #
  # OUTPUTS:
  #   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
  #   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
  
  # Compute posterior hyperparameters
  n = length(y) # Number of observations
  nCovs = dim(X)[2] # Number of covariates
  XX = t(X)%*%X
  betaHat <- solve(XX,t(X)%*%y)
  Omega_n = XX + Omega_0
  mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
  v_n = v_0 + n
  sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
  invOmega_n = solve(Omega_n)
  
  # The actual sampling
  sigma2Sample = rep(NA, nIter)
  betaSample = matrix(NA, nIter, nCovs)
  for (i in 1:nIter){
    
    # Simulate from p(sigma2 | y, X)
    sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
    sigma2Sample[i] = sigma2
    
    # Simulate from p(beta | sigma2, y, X)
    beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
    betaSample[i,] = beta_
    
  }
  return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}

mu_0=rep(0,ncol(X))
omega_0=1/100*diag(ncol(X))
v_0=1
sigma2_0=36
nIter=5000
post_distrib=BayesLinReg(y,X, mu_0, omega_0, v_0, sigma2_0, nIter)
post_beta=post_distrib$betaSample
colnames(post_beta)=covNames
lstat_post=subset(post_beta, select="lstat")
par(mfrow=c(1,1))
plot(density(lstat_post), main="Posterior density of lstat", lwd=2)
credInterval=quantile(lstat_post, probs=c(0.05, 0.95))
abline(v=credInterval[1], col="grey", lwd=3, lty=3)
abline(v=credInterval[2], col="grey", lwd=3, lty=3)

# Since posterior of beta is the student t-distrib the distrib is symmetric and therefore HPD interval is the same
# as equal tail interval

new_obs=X[9,]
names(new_obs)=covNames
new_obs_2=new_obs
new_obs_2[which(names(new_obs)=="lstat")]=new_obs_2[which(names(new_obs)=="lstat")]*0.7
post_sigma2=post_distrib$sigma2Sample
pred_price1=post_beta%*%new_obs+rnorm(nIter, mean=0, sd=sqrt(post_sigma2))
pred_price2=post_beta%*%new_obs_2+rnorm(nIter, mean=0, sd=sqrt(post_sigma2))
hist(pred_price1, breaks=50, main="Histogram of predicted price before change")
hist(pred_price2, breaks=50, main="Histogran of predicted price after change")
pred_price_house9=post_beta[,14]*(new_obs[14]*0.7-new_obs[14])
mean(pred_price_house9)
quantile(pred_price_house9, probs=c(0.025, 0.975))

# For a house like number 9 it will increase the house price with high posterior probability. 

```

##Assignment 4

```{r eval=FALSE}
## a) Simulate 1000 draws from predictive distrib of the maximal weight on a given future day, model: y=10*a where 
## y is the weight and a is the build cost. y~N(theta, sigma2). Noninformative prior assumed.

y=c(191, 196, 197, 189)
sigma2=10^2
# Noninformative prior assumed to be constant

yPred_post=rnorm(1000, mean=mean(y), sd=sqrt(sigma2*(1+1/length(y))))

## b) Use simulation to approximate the predictive probability that weight higher than 230

pred_max365=rep(0,1000)
for (i in 1:1000) {
  pred_max365[i]=max(rnorm(365, mean=mean(y), sd=sqrt(sigma2*(1+1/length(y)))))
}
prob_yPred365=sum(pred_max365>230)/1000
print(prob_yPred365)

## The probability is 0.157

## c) The loss function is linear

expectedLoss = function(a, maxWeight) {
  probCollapse=sum(maxWeight>10*a)/1000
  return(a*(1-probCollapse)+probCollapse*(a+100))
}

a=seq(20,30,0.01)
plot(a, sapply(a, expectedLoss, maxWeight=pred_max365), type="l", lwd=2, xlab="a", ylab="EL",
     main="Loss function")
aOpt=a[which(min(sapply(a, expectedLoss, maxWeight=pred_max365)))]
print(aOpt)

## The answer is 23.89
```

#2018-06-01

##Assignment 1

```{r}
## a) Draw 1000 samples from prior (Gamma) and 1000 samples from posterior (Gamma). Plot prior and posterior using
## both samples and their analytical expressions. 

n=50
x_mean=10
beta=2
nDraws=1000

# We know that posterior distribution is the Gamma(alpha+sum(data), beta+n). Mean for Gamma distrib is alpha/beta.
## If beta=2 then beta+n for posterior is 52. alpha/2=(alpha+500)/52 which yields 50*alpha=1000 and alpha=20
## Check: 20/2=10, (20+500)/52=10 OK!

alpha=20 # According to motivation above
post_draws=rgamma(nDraws, alpha+n*x_mean, beta+n)
prior_draws=rgamma(nDraws, alpha, beta)
gridWidth=0.01
muGrid_post=seq(7,12, gridWidth) # Range taken with inspiration from histogram
muGrid_prior=seq(4,20,gridWidth)
par(mfrow=c(2,1))
hist(post_draws, breaks=50, main="Posterior", xlab=expression(mu),
     freq=FALSE)
lines(muGrid_post, dgamma(muGrid_post, alpha+n*x_mean, beta+n), lwd=2, xlab=expression(mu))
hist(prior_draws, breaks=50, main="Prior", xlab=expression(mu),
     freq=FALSE)
lines(muGrid_prior, dgamma(muGrid_prior, alpha, beta), lwd=2, xlab=expression(mu))

## As seen in the plots the distributions resemble each other. 

## b) Simulate 1000 draws from predictive distribution of new observation and plot distribution.

par(mfrow=c(1,1))
x_pred=rpois(1000, lambda=post_draws)
hist(x_pred, breaks=50, main="Histogram, approximated posterior predictive distribution", xlab=expression(mu),
     freq=FALSE)

## c) Prob that x51=10 based on posterior predictive distribution

sum(x_pred==10)/nDraws

```

##Assignemnt 2

```{r eval=FALSE}
## Linear regression model for fish with 3 covariates. 

# Reading the data from file
load(file = 'fish.RData')

library(mvtnorm)

# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
  # Direct sampling from a Gaussian linear regression with conjugate prior:
  #
  # beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
  # sigma2 ~ Inv-Chi2(v_0,sigma2_0)
  # 
  # Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
  #
  # INPUTS:
  #   y - n-by-1 vector with response data observations
  #   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
  #   mu_0 - prior mean for beta
  #   Omega_0  - prior precision matrix for beta
  #   v_0      - degrees of freedom in the prior for sigma2
  #   sigma2_0 - location ("best guess") in the prior for sigma2
  #   nIter - Number of samples from the posterior (iterations)
  #
  # OUTPUTS:
  #   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
  #   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
  
  # Compute posterior hyperparameters
  n = length(y) # Number of observations
  nCovs = dim(X)[2] # Number of covariates
  XX = t(X)%*%X
  betaHat <- solve(XX,t(X)%*%y)
  Omega_n = XX + Omega_0
  mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
  v_n = v_0 + n
  sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
  invOmega_n = solve(Omega_n)
  
  # The actual sampling
  sigma2Sample = rep(NA, nIter)
  betaSample = matrix(NA, nIter, nCovs)
  for (i in 1:nIter){
    
    # Simulate from p(sigma2 | y, X)
    sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
    sigma2Sample[i] = sigma2
    
    # Simulate from p(beta | sigma2, y, X)
    beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
    betaSample[i,] = beta_
    
  }
  return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}

## a) Plot marginal posterior for each param

y=as.matrix(subset(fish, select="length"))
X=as.matrix(fish[,2:ncol(fish)])
covNames=colnames(X)
mu_0=rep(0,3)
omega_0=0.01*diag(1,3)
v_0=1
sigma2_0=100^2
nIter=5000

linPost=BayesLinReg(y, X, mu_0, omega_0, v_0, sigma2_0, nIter)
betaPost=linPost$betaSample
colnames(betaPost)=covNames
sigma2Post=linPost$sigma2Sample
par(mfrow=c(2,2))
for (i in 1:ncol(betaPost)) {
  hist(betaPost[,i], xlab=paste("Beta",i,sep=""), main=paste("Marginal posterior distribution of beta", i, sep=""))
}
hist(sigma2Post, xlab=expression(sigma), main="Marginal posterior distribution of sigma2")

par(mfrow=c(1,1))

## Construct 90 % equal tail interval for beta1 and interpret it.

quantile(subset(betaPost, select="age"), probs=c(0.05, 0.95))

## It can be concluded that when the age of the fish increases with one unit the length of the fish increases 
## with approximately between 2.284 and 2.960 mm with 90 % posterior probability. 

## d) New experiment fish has been grown in water tank with water temp 30 degrees celsius. Newborn fish have
## have been inserted into the tank at two time points, 30 days ago and 100 days ago. Equal amount of fish
## in the two different ages. You pick up fish randomly from water tank. Do bayesian analysis (using sim methods)
## to determine predictive distrib of the length of the picked up fish. 

x1=c(1,30,30)
x2=c(1,100,30)
x_pred=rep(0,nIter)
for (i in 1:nIter) {
  prob=runif(1)
  if(prob>0.5) {
    x_pred[i]=betaPost[i,]%*%x1+rnorm(1, mean=0, sd=sqrt(sigmaPost[i]))
  } else {
    x_pred[i]=betaPost[i,]%*%x2+rnorm(1, mean=0, sd=sqrt(sigmaPost[i]))
  }
}
hist(x_pred, main="Histogram of predictive distribution of length of fish",
     xlab="Length in mm", freq=FALSE, breaks=50)

```

##Assignment 3

```{r eval=FALSE}
## c) Choose between three models where two of them use Beta prior and the last one assumes p=0.5. Which model 
## should be chosen?

model1=choose(10,3)*gamma(4)*gamma(8)*gamma(2)/gamma(12)
model2=choose(10,3)*gamma(7)*gamma(11)*gamma(8)/(gamma(4)*gamma(4)*gamma(18))
model3=choose(10,3)*0.5^10
model1_norm=model1/sum(c(model1, model2, model3))
model2_norm=model2/sum(c(model1, model2, model3))
model3_norm=model3/sum(c(model1, model2, model3))

```

##Assignment 4

```{r eval=FALSE}
## a) Consider observations with values above 200. Remaining datapoints assumed to be indep. and follow a 
## truncated normal distribution with density specified. L=200 lower truncation point. Write a function in R
## that computes the (unnormalized) log posterior distribution of mu. Use function to plot the posterior distrib
## of mu for the observations greater than 200 in the data vector sulfur. For the plot, use a grid constructed
## in R with seq(100,400,1)

# Reading the data from file
load(file = 'sulfur.RData')

muGrid=seq(100,400,1)
sigma=100
data=sulfur[sulfur>200]

# Constant prior for mu is assumed

logPost = function(data, mu, sigma, L=200) {
  nominator=dnorm((data-mu)/sigma, mean=0, sd=1, log=TRUE)
  denominator=log(sigma)+log(1-pnorm((L-mu)/sigma))
  return(sum(nominator-denominator+0)) # Assumed constant prior which can be set to 1 which in log scale is 0
}

post_mu=exp(sapply(muGrid, logPost, data=data, sigma=sigma))
post_mu_norm=post_mu/sum(post_mu) # Since gridwidth is 1 we don't have to compensate for it
plot(muGrid, post_mu_norm, type="l", lwd=2, main="Posterior distribution of mu", xlab=expression(mu))

library(rstan)
T = length(sulfur)
T_cens = sum(sulfur <= 200)
censData <- list(T=T, T_cens = T_cens, x=sulfur, L=200)

# Model
censModel <- '
data {
  int<lower=0> T;       // Total number of time points
  int<lower=0> T_cens;  // Number of censored time points
  real x[T];            // Partly censored data
  real<upper=max(x)> L; // Lower truncation point
}

parameters {
  real mu;
  real<lower=0> sigma;
  real<upper=L> x_cens[T_cens]; // Censored values
}

model {
  int t_cens = 0;
  for (t in 1:T){
    if (x[t] > L) 
      x[t] ~ normal(mu,sigma);
    else {
      t_cens += 1;
      x_cens[t_cens] ~ normal(mu,sigma);
    }
  }
}
'

## b) Now condiser all data points. Values below 200 being censored. 

fit=stan(model_code=censModel, data=censData)
print(fit)
post_draws=extract(fit)
grid=seq(1,4000,1)
plot(grid, post_draws$mu, type="l", main="Traceplot of mu", xlab=expression(mu), ylab="Value")
plot(grid, post_draws$sigma, type="l", main="Traceplot of sigma", xlab=expression(sigma), ylab="Value")
par(mfrow=c(4,2))
for (i in 1:8) {
  plot(grid, post_draws$x_cens[,i], type="l", main=paste("Traceplot of ", i, "th obs of obs below 200", sep=""),
       xlab=i, ylab="Value") 
}
par(mfrow=c(1,1))

plot(post_draws$mu, post_draws$sigma, type="p", col="grey", main="Joint posterior of mu and sigma",
     xlab=expression(mu), ylab=expression(sigma))

## c) Instead consider time series model. Assume that observations follow an independent normal distrib
## when conditioned on a latent AR(1) process z, but with values of xi below 200 being censored and set to 200.
## Modify the stan code in order to do inference for this model instead. Also put a normal prior on 
## mu~N(300,100^2) Plot the posterior of phi. Also produce a plot that contains both the data and the posterior 
## mean and 95 % credible intervals for the latent intensity z over time. 

StanModel_AR = '
data {
  int<lower=0> T;       // Total number of time points
  int<lower=0> T_cens;  // Number of censored time points
  real x[T];            // Partly censored data
  real<upper=max(x)> L; // Lower truncation point
}

parameters {
  real mu;
  real phi;
  real<lower=0> sigma;
  real<upper=L> x_cens[T_cens]; // Censored values
  vector[T] z;
}

model {
  // Prior
  int t_cens = 0;
  phi ~ uniform(-1,1);
  mu ~ normal(300, 100);
  for (n in 2:T)
    z[n] ~ normal(mu + phi * (z[n-1]-mu), sigma);
    

  // Model/likelihood
  for (t in 1:T){
    if (x[t] > L) 
      x[t] ~ normal(z[t],20);
    else {
      t_cens += 1;
      x_cens[t_cens] ~ normal(z[t],20);
    }
  }
}

generated quantities {
  vector[T] post_mean;
  post_mean = z;
}
'
fitAR=stan(model_code=StanModel_AR, data=censData)
print(fitAR)
post_draws_AR=extract(fitAR)
postPhi=post_draws_AR$phi
postZ=post_draws_AR$post_mean
hist(postPhi, breaks=50, main="Approximated posterior density of phi", xlab=expression(phi), freq=FALSE)
grid=seq(1,31)
plot(grid, sulfur, col="blue", main="Emissions of sulfur dioxide", xlab="Day of month", ylab="mg/Nm^3",
     ylim=c(0,500))
postMean=rep(0,ncol(postZ))
credIntervals=matrix(0,ncol(postZ),2)
for (i in 1:ncol(postZ)) {
  postMean[i]=mean(postZ[,i])
  credIntervals[i,]=quantile(postZ[,i], probs=c(0.025, 0.975))
}
lines(grid, postMean, type="l", col="red", lwd=2)
lines(grid, credIntervals[,1], col="grey", lwd=1, lty=2)
lines(grid, credIntervals[,2], col="grey", lwd=1, lty=2)
legend("topleft", legend=c("Data", "Posterior mean", "95 % cred intervals"), lwd=c(NaN, 2, 1), lty=c(NaN,1,2),
       pch=c(1,NaN, NaN), col=c("blue", "red", "grey"))

```

#2018-08-22

##Assignment 1

```{r eval=FALSE}
## a) Plot posterior density of lognormal likelihood and normal prior

sigma2=0.04
dataLion =lions

posteriorDens = function(data, mu, sigma2, mu0, sigma2_0) {
  likelihood=sum(dlnorm(data, meanlog=mu, sdlog=sqrt(sigma2), log=TRUE))
  prior=dnorm(mu, mean=mu0, sd=sqrt(sigma2_0), log=TRUE)
  return(likelihood+prior)
}

gridWidth=0.001
muGrid=seq(5,5.5,gridWidth)
postMu=exp(sapply(muGrid, posteriorDens, data=dataLion, sigma2=0.04, mu0=5, sigma2_0=1))
postMu_norm=1/gridWidth*postMu/sum(postMu)
plot(muGrid, postMu_norm, type="l", lwd=2, main="Posterior density of mu", xlab=expression(mu), ylab="Density")

## b) Now assume that also sigma2 is unknown and that sigma2 ~ scaledinvchisq(v0, sigma2_0) a priori independently
## from mu, with v0=5 and sigma2_0=0.04. Implement stan-code that produces at least 2000 samples from the posterior
## of mu and sigma2. Use 500 samples for burnin. Based on samples compute posterior mean and standard deviation
## of mu and sigma2 and plot the joint posterior of mu and sigma2. 

StanModel= '
data {
  int<lower=0> N;
  vector[N] y;
}

parameters {
  real mu;
  real<lower=0> sigma2;
}
model {
  //Priors
  mu ~ normal(5,1);
  sigma2~scaled_inv_chi_square(5,0.2);
  
  //Likelihood
  for (n in 1:N) {
    y[n]~lognormal(mu, sqrt(sigma2));
  }
}
'
n=length(dataLion)
data=list(N=n, y=dataLion)
fit=stan(model_code=StanModel, data=data, warmup=500, iter=2500, chains=1)
print(fit)
post_draws=extract(fit)
mu_post=post_draws$mu
sigma2_post=post_draws$sigma2
mu_postMean=mean(mu_post)
sigma2_postMean=mean(sigma2_post)
mu_postMean
sigma2_postMean
plot(mu_post, sigma2_post, main="Simulated joint posterior of mu and sigma", xlab=expression(mu),
     ylab=expression(sigma))

## c) Compute an estimate of the average weight of male lions. Give an estimate and a 95 % credible interval of 
## the average weight of male lions based on the posterior computed in b. 

estimate=exp(mu_post+1/2*sigma2_post)
mean(estimate)
hist(estimate, breaks=100, main="Predicitve distribution of average weight of lions")
quantile(estimate, probs=c(0.025, 0.975))

## Important here to insert all the draws into the function for the mean and then take the average of that for
## an estimate. 
```

##Assignment 2

```{r eval=FALSE}
## a) Use numeric optimization to approximate joint posterior of beta.

library(mvtnorm)

data=titanic
y=data$survived
X=as.matrix(data[,-1])


nFeatures = dim(X)[2]
covNames=names(data[,2:ncol(data)])

# Constructing prior
tau=50
mu_prior = rep(0,nFeatures)
sigma_prior = tau^2*diag(nFeatures) 

logPostLogistic = function(beta, Y, X, mu, sigma) {
  nFeat = length(beta)
  XBeta=X%*%beta
  # Defining loglikelihood
  logLike = sum(Y*XBeta-log(1+exp(XBeta)))
  if (abs(logLike) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
  # Defining prior
  prior = dmvnorm(beta, mean=mu, sigma=sigma, log=TRUE)
  # Adding loglikelihood and logprior together. Since it is log both of them are added instead of multiplied
  return(logLike + prior)
}

# Defining initial values to be passed on to the optimizer
set.seed(12345)
initVals = rnorm(dim(X)[2])

# Finding the optimized betavector
optimResult = optim(initVals, logPostLogistic, Y=y, X=X, mu=mu_prior, sigma=sigma_prior, method=c("BFGS"),
                    control=list(fnscale=-1), hessian=TRUE)

# Defining the values of interest
postMode = optimResult$par
postCov = -solve(optimResult$hessian)
names(postMode) = covNames
approx_PostStd = sqrt(diag(postCov))
names(approx_PostStd) = covNames
print("The posterior mode is:")
print(postMode)
print("The approximated standard deviations are:")
print(approx_PostStd)

par(mfrow=c(2,2))
for (i in 2:nFeatures) {
  grid=seq(postMode[i]-3*approx_PostStd[i], postMode[i]+3*approx_PostStd[i], length=1000)
  plot(grid, dnorm(grid, mean=postMode[i], sd=approx_PostStd[i]), 
       main=paste("Marginal posterior of", covNames[i]), xlab=covNames[i], ylab="Density", type="l", lwd=2)
}

## b) Compute posterior probability that the adult feature is smaller than 0

prob=pnorm(0, mean=postMode[which(covNames=="adult")], sd=approx_PostStd[which(covNames=="adult")])
prob

## The interpretation of the probability 0.76 is that we can say with approximately 76 % posterior probability
## that being an adult contributed negatively in regards to survival on the titanic. If you were an adult 
## you were more probable to die than if not.

## c) A first class adult woman and a third class adult man are together during the disaster.
## Compute predictive probability that the woman survives but the man dies. 

man=c(1,1,1,0,0)
woman=c(1,1,0,1,0)
nDraws=5000
results=matrix(0,nDraws, nFeatures)
for (i in 1:nDraws) {
  results[i,]=rmvnorm(1, mean=postMode, sigma=postCov)
}

manPred=results%*%man
womanPred=results%*%woman
manSim=rbinom(nDraws, 1, exp(manPred)/(1+exp(manPred)))
womanSim=rbinom(nDraws, 1, exp(womanPred)/(1+exp(womanPred)))
final=ifelse(womanSim == 1 & manSim ==0, 1, 0)
mean(final)

## Reasonable. Do simulation of param, use that param in new obs likelihood. Check probability.
```

##Assignment 3

```{r eval=FALSE}
## c) Do Bayesian model comparison of two models, both geometric likelihood, first with beta prior alpha=0.5, beta=0.5
##  and second is null model assuming theta=0.5. Prior probabilities are p(M1)=0.1 and p(M2)=9/10

alpha=1/2
beta=1/2
data=c(2, 1, 12)
n=length(data)

# We know that marginal likelihood for data is likelihood*prior/posterior which when derived yields the below function

margLikelihood1=gamma(alpha+beta)*gamma(alpha+n)*gamma(beta+sum(data))/(gamma(alpha)*gamma(beta)*
                                                                          gamma(beta+sum(data)+alpha+n))

# Since we have a constant prior the marginal likelihood for model 2 will be the likelihood with theta set to the 
## constant

margLikelihood2=0.5^n*(1-0.5)^sum(data)

prior1=1/10
prior2=9/10

posterior1=margLikelihood1*prior1
posterior2=margLikelihood2*prior2
posterior1_norm=posterior1/sum(c(posterior1, posterior2))
posterior2_norm=posterior2/sum(c(posterior1, posterior2))

## Model 1 has a higher posterior probability than 1 so this model should be chosen. 
```

##Assignment 4

```{r eval=FALSE}
## b) Compute the probability that party A gets a majority of the votes (more than 50 %) in the election. 
## Assume that everyone in the population is voting. 

# Assuming uniform Dirichlet prior, i.e. Dirichlet(1,1,1)
nComp=3
alphaPrior=rep(1,nComp)
data=c(184, 67, 149)
alphaPost=alphaPrior+data

simDirich = function(alpha) {
  nComp=length(alpha)
  gammavec=rep(0,nComp)
  for (i in 1:nComp) {
    x=rgamma(1, alpha[i], 1)
    gammavec[i]=x
  }
  z=gammavec/sum(gammavec)
  return(z)
}

nIter=10000
dirichlet=matrix(0, nIter, nComp)
for (i in 1:nIter) {
  dirichlet[i,]=simDirich(alphaPost)
}

prob=mean(dirichlet[,1]>0.5)
prob

## 0.039 % that party A gets a majority of the votes

## Calc probability that A becomes largest party

prob2=mean(dirichlet[,1]>dirichlet[,2] & dirichlet[,1]>dirichlet[,3])
prob2

## 0.971 % chance that party A becomes the biggest party.

## d) Assume that probability in c) was estimated through monte carlo simulation. Compute a 95 % confidence interval
## for estimated probility in c) with respect to the error from the Monte Carlo simulation. 

# If x modeled as Bin(1,p) where p is the probability obtained from c) and x=1 stands for {A becomes largest party}
# and x=0 stands for {A doesn't become largest party}. We get that the expected value of the summation of all
# monte carlo samples divided by the number of samples is p and the variance for same variable is p(1-p)/#Samples

p=prob2
N=10000
stdX=p*(1-p)/N
confInt=c(p-1.96*sqrt(1/N*p*(1-p)), p+1.96*sqrt(1/N*p*(1-p)))
confInt

## How many additional samples would be needed to reduce the width of the interval by half

# To reduce width by half the difference between upper and lower bound of interval divided by 2 needs to be 
# reduced by half. This yields: p+1.96*1/sqrt(N)*sqrt(p*(1-p))-(p-1.96*1/sqrt(N)*sqrt(p*(1-p)))/2=
# 1.96*..., 1.96/2*1/sqrt(N)...=1.96*1/sqrt(N)... So we need to increase N to N~ to obtain the left hand side
# expression. We get that 1/sqrt(N~)=1/(2*sqrt(N)), N~=4*N

NTilde=4*10000
diffN=NTilde-N
diffN

## We need to increase the sample by 30000 to lower the interval by half. 
```

#2018-11-01

##Assignment 1

```{r eval=FALSE}
## a) Data is normally distributed, assume non-informative prior. Simulate 1000 draws from the predictive distribution
## of the maximal weight in a given future week and plot them.

par(mfrow=c(1,1))
data=c(1690, 1790, 1760, 1750)
n=length(data)
sigma2=50^2

# Prior assumed to be constant. This yields posterior distribution of mu~N(mean(data), simga2/n) according to L2

postDistrib=rnorm(1000, mean=mean(data), sd=sqrt(sigma2/n))
predDistrib=rnorm(1000, mean=postDistrib, sd=sqrt(sigma2))
hist(predDistrib, breaks=100, main="Approximated predictive distribution of mu", xlab=expression(mu), freq=FALSE)

# To check if reasonable the real predictive distribution is plotted. We know from L4 that the predictive distribution
# of new obs is distributed N(mean(data), sigma2*(1+1/n))

grid=seq(1500,1900)
lines(grid, dnorm(grid, mean=mean(data), sd=sqrt(sigma2*(1+1/n))), col="red")

## Since the histogram follows the real distribution well it was performed correctly. 

## b) Use simulation to approximate the expected number of weeks out of the coming 52 weeks in which the maximal 
## weight will exceed 1850 kg, based on the predictive distribution. 

nDraws=1000
weekMatrix=matrix(0,52,nDraws)
for (i in 1:nDraws) {
  weekMatrix[,i]=rnorm(52, mean=mean(data), sd=sqrt(sigma2*(1+1/n)))
}
countWeeks=colSums(weekMatrix>1850)
barplot(table(countWeeks), main="Approximated predictive distribution", xlab="No. of weeks")
mean(countWeeks)

## Important here to simulate the number of predictive draws taken, in this case 52 samples. Then sum the no of
## observations in each sample which satisfies the condition. This then becomes the predictive distribution.
## We can then take the mean out of this sample to obtain the expected number of weeks. 

## c) The weight that the escalator can hold at any given time is given by 1000log(a), a is the build cost.
## If the weight is exceeded the excalator breaks and has to be repaired. Loss function for shopping mall is
## L(a, theta) = a+n(a,theta) where n(a,theta) is the no. of weeks out of the 52 in which the escalator breaks.
## Compute the optimal build cost (a) using Bayesian approach.

# Want to maximize the negative loss function.

countOfBreak=function(a, countMatrix) {
  return(colSums(countMatrix>1000*log(a)))
}

utilityFunction = function(a, n) {
  return(-(a+n))
}

aGrid=seq(0,20, 0.001)
utility=rep(0,length(aGrid))
for(i in 1:length(aGrid)) {
  counts=countOfBreak(aGrid[i], weekMatrix)
  utility[i]=mean(utilityFunction(aGrid[i], counts))
}

plot(aGrid, utility, type="l", lwd=2, main="Utility function")
aOpt=aGrid[which(utility==max(utility))]
points(aOpt, max(utility), col="red", cex=2, lwd=2)
aOpt

## 6.74 yields maximum utility and equivalently minimum loss.
```

##Assignment 2

```{r eval=FALSE}
# Reading the data from file
load(file = 'fish.RData')

library(mvtnorm)

# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
  # Direct sampling from a Gaussian linear regression with conjugate prior:
  #
  # beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
  # sigma2 ~ Inv-Chi2(v_0,sigma2_0)
  # 
  # Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
  #
  # INPUTS:
  #   y - n-by-1 vector with response data observations
  #   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
  #   mu_0 - prior mean for beta
  #   Omega_0  - prior precision matrix for beta
  #   v_0      - degrees of freedom in the prior for sigma2
  #   sigma2_0 - location ("best guess") in the prior for sigma2
  #   nIter - Number of samples from the posterior (iterations)
  #
  # OUTPUTS:
  #   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
  #   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
  
  # Compute posterior hyperparameters
  n = length(y) # Number of observations
  nCovs = dim(X)[2] # Number of covariates
  XX = t(X)%*%X
  betaHat <- solve(XX,t(X)%*%y)
  Omega_n = XX + Omega_0
  mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
  v_n = v_0 + n
  sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
  invOmega_n = solve(Omega_n)
  
  # The actual sampling
  sigma2Sample = rep(NA, nIter)
  betaSample = matrix(NA, nIter, nCovs)
  for (i in 1:nIter){
    
    # Simulate from p(sigma2 | y, X)
    sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
    sigma2Sample[i] = sigma2
    
    # Simulate from p(beta | sigma2, y, X)
    beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
    betaSample[i,] = beta_
    
  }
  return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}

data=fish
y=data[,1]
X=as.matrix(data[,-1])
X=cbind(X, subset(X, select=c("age", "temp"))^2)
X=cbind(X, X[,2]*X[,3])
covNames=names(data[,-1])
covNames=append(covNames, c("age^2", "temp^2", "age*temp"))
mu_0=rep(0, ncol(X))
omega_0=0.01*diag(ncol(X))
v_0=1
sigma2_0=10000
nIter=5000

bayes_lin_results=BayesLinReg(y, X, mu_0, omega_0, v_0, sigma2_0, nIter)

## a) Compute posterior mean and 95 % equal tail credible intervals for all beta-params

results=matrix(0,ncol(X)+1,3)
for (i in 1:ncol(X)) {
  results[i,1]=mean(bayes_lin_results$betaSample[,i])
  results[i,-1]=quantile(bayes_lin_results$betaSample[,i], probs=c(0.025, 0.975))
}
results[ncol(X)+1,1]=mean(bayes_lin_results$sigma2Sample)
results[ncol(X)+1,-1]=quantile(bayes_lin_results$sigma2Sample, probs=c(0.025, 0.975))
covNames=append(covNames, "sigma2")
rownames(results)=covNames
colnames(results)=c("Posterior mean", "2,5%", "97,5%")
results

## b) Compute the posterior mean and posterior median of the noise standard deviation theta

median(bayes_lin_results$sigma2Sample)
results[ncol(X)+1,1]

## Results shown above

## c) First eleven datapoints come from watertank with 25 degrees celsius. Produce scatter plot of these datapoints
## with length and age on the two axes. Overlay a curve for the posterior mean of the regression curve with respect
## to age. 

betaMatrix=bayes_lin_results$betaSample
tempData=data[1:11,]
plot(tempData$length, tempData$age, main="Plot of data with 25 degrees temperature in tank", xlab="Length",
     ylab="Age", col="blue")
ageGrid=seq(0,160, 0.01)
credInt=matrix(0,length(ageGrid),2)
fAgePostMean=rep(0,length(ageGrid))
fAge=rep(0,nIter)
count=1
for (a in ageGrid) {
  fAge=betaMatrix%*%c(1,a,25,a^2, 25^2, a*25)
  fAgePostMean[count]=mean(fAge)
  credInt[count,]=quantile(fAge, probs=c(0.025, 0.975))
  count=count+1
}
lines(fAgePostMean, ageGrid, type="l", lwd=2, col="red")
lines(credInt[,1], ageGrid, col="grey", lty=2)
lines(credInt[,2], ageGrid, col="grey", lty=2)

## d) Assume that you want to make predictions for fish in a new water tank with a temperature of 15 degrees celsius
## , which is lower than any of the temperatures in the original data set. Discuss how the current data set
## might be a problem regarding this matter and how the prior could be changed to control this problem. 

## Since we have a model with high order terms the risk for overfitting is bigger than with a simpler model.
## When testing the model on data which are far away from the data used to fit the model, it is of high risk
## that the model might perform badly. To reduce the risk of overfitting one can use a betaprior close to zero
## to force many of the covariates to become zero. To further reduce the risk for overfitting one can increase the
## values in the diagonal of omega_0 to larger values to further reduce the variance. This should be done for the 
## covariates and not for the intercept so the place [1,1] in the omega_0 matrix can remain the same but the other
## values can be increased. 
```

##Assignment 4

```{r eval=FALSE}
## a) Assume following joint prior p(alpha, beta) 1/(alpha*beta)^2

# Reading the data from file
load(file = 'weibull.RData')
library(mvtnorm)

data=weibull

logPrior=function(alphaBeta) {
  return(-2*(log(alphaBeta[1])+log(alphaBeta[2])))
}

logPosterior = function(alphaBeta, data) {
  alpha=alphaBeta[1]
  beta=alphaBeta[2]
  logPrior=logPrior(alphaBeta)
  logLike=sum(dweibull(data, alpha, beta, log=TRUE))
  logPost=logLike+logPrior
  if (abs(logPost) == Inf || is.na(logPost)) logPost = -20000;
  return(logPost)
}

# Defining initial values to be passed on to the optimizer
set.seed(12345)
initVals = rnorm(2)

optimResult = optim(initVals, logPosterior, data=data, method=c("L-BFGS-B"),
                    lower=c(0.0001,0.0001), upper=c(Inf, Inf), control=list(fnscale=-1), hessian=TRUE)

# Defining the values of interest
postMode = optimResult$par
postCov = -solve(optimResult$hessian)
names(postMode) = c("alpha", "beta")
approx_PostStd = sqrt(diag(postCov))
names(approx_PostStd) = c("alpha", "beta")
colnames(postCov) = c("alpha", "beta")
rownames(postCov) = c("alpha", "beta")
print("The posterior mode is:")
print(postMode)
print("The covariance matrix is:")
print(postCov)

## b) Simulate from the actual posterior using the metropolis algorithm. Denote theta=t(alpha,beta) and use
## proposal density the multivariate normal density (random walk metropolis). Use alpha=1 and beta=1 as starting
## values, 500 iterations burnin and thereafter 2000 samples from the posterior. Run the algorithm for c: 0.1,
## 4 and 100 and use the draws from the best choice of c. Motivate your choice. Compute posterior mean and variance
## for the two params based on your samples. Proposal distrib can be truncated to avoid proposals below zero.

nIter=2000
burnIn=500
alphaGamma=c(1,1)
c=c(0.1, 4, 100)

# Defining function for sampling through metropolishastings
RVMSampler = function(previousVal, postCov, c, myFunction, ...) {
  proposalVal=rmvnorm(1, mean=previousVal, sigma=c*postCov)
  proposalVal[proposalVal<=0]=1e-6
  alpha=min(1, exp(myFunction(proposalVal,...)-myFunction(previousVal, ...)))
  u=runif(1)
  if(u < alpha) {
    return(list(theta=proposalVal, acceptProb=alpha))
  } else {
    return(list(theta=previousVal, acceptProb=alpha))
  }
}

post_matrix = matrix(0, nIter+burnIn, 2*length(c))
colnames=c()
for (i in 1:length(c)) {
  colnames=c(colnames, c(paste("Alpha with c=", c[i], sep=""), paste("Beta with c=", c[i], sep="")))
}
# Setting initial values of beta to same initVals as in the optimizer (taken randomly from normal distrib)
post_matrix[1,]=alphaGamma
accProb=matrix(0, nIter+burnIn, length(c))
colnames(accProb)=c("c=0.1", "c=4", "c=100")
set.seed(12345)

for (j in 1:length(c)) {
  for(i in 1:(nIter+burnIn)) {
    if(i<(nIter+burnIn)) {
      draw=RVMSampler(post_matrix[i,(2*j-1):(2*j)], postCov, c[j], logPosterior, data)
      post_matrix[i+1,(2*j-1):(2*j)]=draw$theta
      accProb[i+1,j]=draw$acceptProb
    }
  }
}
accProb_final=accProb[-(1:burnIn),]
accProb_mean=colMeans(accProb)
accProb_mean
colnames(post_matrix)=colnames

## Should choose c=4 since it yields an acceptance probability of around 0.3 which is a preferable value when
## applying metropolis hastings algorithm. 

post_matrix_final=post_matrix[-(1:burnIn), 3:4]
rownames=seq(501,2500)
rownames(post_matrix_final)=rownames
postMean_alpha=mean(post_matrix_final[,1])
postMean_beta=mean(post_matrix_final[,2])
postVar_alpha=var(post_matrix_final[,1])
postVar_beta=var(post_matrix_final[,2])
postMean_alpha
postMean_beta
postVar_alpha
postVar_beta
```

#2019-06-04

##Assignment 1

```{r eval=FALSE}
## Bayesian data analysis before an upcoming election. According to model, the posterior of the vote share theta
## that the party will get in the election is Beta(sqrt(c), 20) distributed, where c is the amount (in million SEK)
## that the party spends on the compaign. 
## a) Sample 10000 draws from posterior of thetafor the cases when c=4 and c=16. Based on the samples, plot the 
## posterior of theta for both cases.

postTheta_4=rbeta(10000, sqrt(4), 20)
postTheta_16=rbeta(10000, 4, 20)
hist(postTheta_4, breaks=50, main="Approximated posterior of theta", sub="c=4", xlab=expression(theta), freq=FALSE)
hist(postTheta_16, breaks=50, main="Approximated posterior of theta", sub="c=16", xlab=expression(theta), freq=FALSE)

## Compute probability that party gets at least 10 % of the votes for both cases. 

prob_4=sum(postTheta_4>0.1)/10000
prob_16=sum(postTheta_16>0.1)/10000
prob_4
prob_16

## To evaluate election results, utility function u(theta, c)=100+20log(theta)-c is used. How much money should
## they spend on the campaign? Do at least 10000 draws. Assume that maximum campaign budget is 20 million SEK.
## Consider values of c on the grid seq(4,20,0.5)

cGrid=seq(4,20,0.5)
postC=matrix(0,10000,length(cGrid))
set.seed(12345)
utility = function(c) {
  thetaDraws=rbeta(10000, sqrt(c), 20)
  return(100+20*log(thetaDraws)-c)
}
# Since quadratic loss function, choose posterior mean
postMean=rep(0,length(cGrid))
for(i in 1:length(cGrid)) {
  postC[,i]=utility(cGrid[i])
  postMean[i]=mean(postC[,i])
}

plot(cGrid, postMean, main="Posterior mean of expected utility", xlab="c", ylab="Utility", type="l", lwd=2)
optC=cGrid[which(postMean==max(postMean))]
abline(v=optC, col="blue", lwd=1, lty=2)
legend("topright", legend=c("Utility function", "Optimal c"), col=c("black", "blue"), lty=c(1,2), lwd=c(2,1))
optC

## Optimal decision is to invest 10.5 million SEK into the campaign
```


##Assignment 2

```{r eval=FALSE}
## Using dataset ebay. Data describing the number of bids of 100 ebay auctions. 
## a) Assume bimodial model with N=50. Use prior (theta-1)^2.
## Write function in R that computes (unnormalized) log posterior density function of theta. Use function to 
## plot normalized posterior density function of theta on the interval [0,1] with at least 1000 grid points. 
## Report the (approximate) value of the posterior mode based on the computed values needed for the plot.

N=50
n=100
data=ebay

logPrior = function(theta) {
  return(log((theta-1)^2))
}

# The likelihood is proportional to the Beta(sum(data)+1, Nn-sum(data)+1) density
logLike = function(data, theta, N=50) {
  return(dbeta(theta, sum(data)+1, N*length(data)-sum(data)+1, log=TRUE))
}

logPost = function(data, theta, N=50) {
  log_Prior=logPrior(theta)
  log_Like=logLike(data, theta, N)
  return(log_Prior+log_Like)
}

thetaGrid=seq(0,1,0.001)
post_theta=exp(sapply(thetaGrid, logPost, data=data, N=50))
post_theta_norm=1/0.001*post_theta/sum(post_theta)
plot(thetaGrid, post_theta_norm, type="l", lwd=2, main="Approximated posterior density of theta", 
     xlab=expression(theta), ylab="Density")
postMode=thetaGrid[which(post_theta_norm==max(post_theta_norm))]
print(postMode)     
abline(v=postMode, col="red", lty=2)
title(sub="Black = Density, Red = Posterior mode")

## b) Use supplied function GibbsMixPoisin file ExamData to do Gibbs sampling for a mixture of Poissons model
## where each data pointis modeled as independent with density given.

set.seed(100)
K=2
nIter=500
xGrid=seq(min(data), max(data))
results=GibbsMixPois(ebay, K, alpha=1, alphaGamma=1, betaGamma=1, xGrid=xGrid, nIter)
post_theta=results$thetaSample
cum_mean=matrix(0,nIter,2)
theta1_cumsum=cumsum(post_theta[,1])
theta2_cumsum=cumsum(post_theta[,2])
for (i in 1:nIter) {
  cum_mean[i,]=c(theta1_cumsum[i]/i, theta2_cumsum[i]/i)
}
par(mfrow=c(2,1))
plot(seq(1,500), post_theta[,1], xlab="No. of bids", ylab=expression(theta), main="Trace plot", type="l")
title(line=3, main="Convergence of sampler for theta1")
plot(seq(1,500), cum_mean[,1], xlab="No. of bids", ylab="Cumulative mean", type="l", main="Cumulative means")
plot(seq(1,500), post_theta[,2], xlab="No. of bids", ylab=expression(theta), main="Trajectory over theta2", type="l")
title(line=3, main="Convergence of sampler for theta2")
plot(seq(1,500), cum_mean[,2], xlab="No. of bids", ylab="Cumulative mean", type="l", main="Cumulative means")

## According to plots we should choose burnin = 50 approximately. 

## c) Use graphical methods to investigate if mixture of Poissons with K=2 fits data well.

data_norm=as.vector(bidsCounts/sum(bidsCounts))
postMean=results$mixDensMean
par(mfrow=c(1,1))
plot(xGrid, data_norm, xlab="No. of bids", ylab="Density", main="Fitted models", type="o", lwd=2, ylim=c(0,0.3))
lines(xGrid, postMean, col="red", lwd=1, type="o", lty=2)
lines(xGrid, dbinom(xGrid, 50, postMode), col="blue", lwd=1, type="o", lty=2)
legend("topright", col=c("black", "red", "blue"), 
       legend=c("Data", "Posterior mean of mixture model", "Binomial model"), 
       lty=c(1,2,2), lwd=c(2,1,1), pch=c("o", "o", "o"))

## I would recommend mixture of poissons since it fits data better. The binomial model is clearly worse 
## specifically when we look at the number of 0 bids which is higher in the data.
```

##Assignment 4

```{r eval=FALSE}
## a) Exp model with Gamma prior. Which prior is more informative, Gamma(2,1) or Gamma(10,10)?

thetaGrid=seq(0,20,0.01)
par(mfrow=c(2,1))
plot(thetaGrid, dgamma(thetaGrid, 2, 1), type="l", lwd=2, main="Prior distrib of Gamma(2,1)", xlab=expression(theta),
     ylab="Density")
plot(thetaGrid, dgamma(thetaGrid, 10, 10), type="l", lwd=2, main="Prior distrib of Gamma(10,10)", xlab=expression(theta),
     ylab="Density")
par(mfrow=c(1,1))

## As seen in the graph the prior for theta in the plot below is more informative since it has a tighter peak than
## the graph above it. This means that it is more probable that theta is a specific value whereas in the above plot
## the probability is more spread over a larger interval of possible theta values. 

## b) Compute marginal likelihood for the two models. 

data=cellphones

margLikelihood = function(data, alpha, beta) {
  n=length(data)
  nominator=beta^alpha*gamma(alpha+n)
  denominator=gamma(alpha)*(beta+sum(data))^(alpha+n)
  return(nominator/denominator)
}

margLike1=margLikelihood(data, 2, 1)
margLike2=margLikelihood(data, 10, 10)
postModel1=margLike1*0.5
postModel2=margLike2*0.5
postModel1_norm=postModel1/sum(c(postModel1, postModel2))
postModel2_norm=postModel2/sum(c(postModel1, postModel2))

## Model 1 is more probable and should be chosen!

## c) Compute 90 % posterior predictive interval of x~ given the cellphones dataset. 

predLikelihood = function(data, alpha, beta, xTilde) {
  n=length(data)
  nominator=(beta+sum(data))^(alpha+n)
  denominator=(beta+xTilde+sum(data))^(alpha+n+1)
  return(nominator*(alpha+n)/denominator)
}

xTilde=seq(0,20, 0.01)
xTilde1=sapply(xTilde, predLikelihood, data=data, alpha=2, beta=1)
xTilde2=sapply(xTilde, predLikelihood, data=data, alpha=10, beta=10)
post_xTilde=postModel1_norm*xTilde1+postModel2_norm*xTilde2  

ndraws = 1000000
xTildeDraws = rep(0,ndraws)
for(i in 1:ndraws){
  M = rbinom(1,1,postModel2_norm) + 1 # Simulate which model to use
  if(M==1){
    theta = rgamma(1,shape=2+length(x),rate=1+sum(x))
  } else {
    theta = rgamma(1,shape=10+length(x),rate=10+sum(x))
  }
  xTildeDraws[i] = rexp(1,theta)
}

print(quantile(xTildeDraws,probs = c(.05,.95)))
hist(xTildeDraws, freq=FALSE, breaks=1000)
lines(xTilde, post_xTilde, type="l", lwd=2, col="red")
plot(xTilde, post_xTilde, type="l", lwd=2)
```

#2019-08-21

##Assignment 1

```{r eval=FALSE}
## a) Use BayesLinReg to sim 5000 draws from posterior distrib of all coeff coefficients. Summarize posterior
## with point estimate under quadratic loss function and 95 % equal tail intervals. Interpret cred intervals for
## regression coefficient on nitrogen oxides concentration.

###############################
########## Problem 1 ########## 
############################### 

# Reading the data from file
library(MASS)
BostonHousing = Boston
y = BostonHousing$medv
X = cbind(1,BostonHousing[,1:13]) # Adding a column of ones for the intercept
names(X)[1] <- "intercept"
covNames <- names(X)
y <- as.numeric(y)
X <- as.matrix(X)
XNewHouse <- c(1,0.03,40,1.5,0,0.5,6,30,5,3,300,17,390,4)

if(length((grep("mvtnorm",installed.packages()[,1])))==0)
  install.packages("mvtnorm")
library(mvtnorm)

# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
  # Direct sampling from a Gaussian linear regression with conjugate prior:
  #
  # beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
  # sigma2 ~ Inv-Chi2(v_0,sigma2_0)
  # 
  # Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
  #
  # INPUTS:
  #   y - n-by-1 vector with response data observations
  #   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
  #   mu_0 - prior mean for beta
  #   Omega_0  - prior precision matrix for beta
  #   v_0      - degrees of freedom in the prior for sigma2
  #   sigma2_0 - location ("best guess") in the prior for sigma2
  #   nIter - Number of samples from the posterior (iterations)
  #
  # OUTPUTS:
  #   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
  #   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
  
  # Compute posterior hyperparameters
  n = length(y) # Number of observations
  nCovs = dim(X)[2] # Number of covariates
  XX = t(X)%*%X
  betaHat <- solve(XX,t(X)%*%y)
  Omega_n = XX + Omega_0
  mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
  v_n = v_0 + n
  sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
  invOmega_n = solve(Omega_n)
  
  # The actual sampling
  sigma2Sample = rep(NA, nIter)
  betaSample = matrix(NA, nIter, nCovs)
  for (i in 1:nIter){
    
    # Simulate from p(sigma2 | y, X)
    sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
    sigma2Sample[i] = sigma2
    
    # Simulate from p(beta | sigma2, y, X)
    beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
    betaSample[i,] = beta_
    
  }
  return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}

mu_0=rep(0, ncol(X))
omega_0=1/10^2*diag(ncol(X))
v_0=1
sigma2_0=5^2
nIter=5000
linPost=BayesLinReg(y, X, mu_0, omega_0, v_0, sigma2_0, nIter)
betaPost=linPost$betaSample
sigma2Post=linPost$sigma2Sample
results=matrix(0,ncol(X)+1,3)
results_names=covNames
results_names=append(results_names, "sigma2")
rownames(results)=results_names
colnames(results)=c("Point estimator", "2,5%", "97,5%")
for (i in 1:ncol(X)) {
  results[i,1]=mean(betaPost[,i])
  results[i,-1]=quantile(betaPost[,i], probs=c(0.025, 0.975))
}
results[(ncol(X)+1),1]=mean(sigma2Post)
results[(ncol(X)+1),-1]=quantile(sigma2Post, probs=c(0.025, 0.975))
results

## b) Kernel density estimates. Compute posterior mode and HPD 90 % for sigma2

sigma2_kernel=density(sigma2Post)
sigma2_kernel.df=data.frame(sigma2=sigma2_kernel$x, density=sigma2_kernel$y)
sigma2_kernel.df=sigma2_kernel.df[order(-sigma2_kernel.df[,2]),]
index=dim(sigma2_kernel.df)[1]
sigma2_kernel.df$density=cumsum(sigma2_kernel.df$density)/sum(sigma2_kernel.df$density)
sigma2Cred=sigma2_kernel.df[sigma2_kernel.df$density<0.9,]
credInterval=c(min(sigma2Cred$sigma2), max(sigma2Cred$sigma2))
sigma2Mode=sigma2_kernel.df[1,]$sigma2

plot(sigma2_kernel, type="l", lwd=2, main="Kernel density estimate of sigma2", xlab=expression(sigma^2))
abline(v=sigma2Mode, col="red", lwd=1, lty=2)
abline(v=credInterval[1], col="grey", lwd=1, lty=3)
abline(v=credInterval[2], col="grey", lwd=1, lty=3)
legend("topright", legend=c("Kernel density estimate", "Posterior mode", "90 % HPD Interval"), lty=c(1,2,3),
       lwd=c(2,1,1), col=c("black", "red", "grey"))

## c) Construction company planning to build a new house with covariates given in XNewHouse. Cost is 20000 dollars
## and the company is planning to sell the house when finished. Do Bayesian analysis to determine how probable
## it is that the company will make money (that the house will sell for more than 20000 dollars).

XNewHouse <- c(1,0.03,40,1.5,0,0.5,6,30,5,3,300,17,390,4)
profitVec=rep(0,nIter)
for (i in 1:nIter) {
  profitVec[i]=-20+betaPost[i]%*%XNewHouse+rnorm(1, mean=0, sd=sqrt(sigma2Post[i]))
}
hist(profitVec)
probProfit=sum(profitVec>0)/nIter
print(probProfit)
quantile(profitVec, probs=c(0.025, 0.975))

## Very probable that the company will make a profit since 98.82 % of the posterior draws are above zero. Negative
## values are also not present in the 95 % equal tail interval which also indicates that the company will make
## a profit. 

```

##Assignment 2

```{r eval=FALSE}
## b) Simulate predictive draw of max no. of years until next earthquake occurs, 95 % prob. alpha=1, beta=1. 

alpha=1
beta=1
xObs=c(35, 14, 4, 10, 2)
n=length(xObs)i
nIter=5000
predDistrib=rep(0,nIter)
for(i in 1:nIter) {
  posteriorDraw=rbeta(1,alpha+n, beta+sum(xObs))
  predDistrib[i]=rgeom(1,posteriorDraw)
}
predDistrib_maxYear=quantile(predDistrib, probs=0.95)
predDistrib_maxYear
```

##Assignment 3

```{r eval=FALSE}
## c) Calc unnormalized posterior and plot normalized posterior. Gamma prior and indep likelihoods.

gridWidth=0.01
thetaGrid=seq(0,2,gridWidth)
xData <- c(1.888, 2.954, 0.364, 0.349, 1.090, 7.237)
yData <- c(-1.246, -1.139, -0.358, -1.308, -0.930, -0.157, -0.111, -0.635)
alpha=3
beta=2

logPosteriorX = function(theta, alpha, beta) {
  return(dgamma(theta, alpha, beta, log=TRUE))
}

likeY = function(y, theta) {
  return(-3*sum(log(1+(1/5)*(y-log(theta))^2)))
}

logPosterior = function(theta, alpha, beta, xDat, yDat) {
  likelihoodY=likeY(yDat, theta)
  logPostX=logPosteriorX(theta, length(xDat+3), sum(xDat)+2)
  return(likelihoodY+logPostX)
}

post_theta=sapply(thetaGrid, logPosterior, alpha=alpha, beta=beta, xDat=xData, yDat=yData)
post_theta_norm=1/gridWidth*exp(post_theta)/sum(exp(post_theta))
plot(thetaGrid, post_theta_norm, type="l", lwd=2, main="Posterior of theta", xlab=expression(theta),
     ylab="Density")
```

##Assignment 4

```{r eval=FALSE}
## Aircraft incidents assumed to be independent, follow negative binomial distrib. Assume joint prior 
## 1/phi^2
## a) Simulate from posterior using Metropolis algorithm. Denote theta=c(mu, phi) and use as proposal dens
## the multivariate normal density (random walk metropolis).

# Load airline incidents data
load(file = 'incidents.RData')
data=incidents$incidents
library(mvtnorm)

nIter=1000
burnIn=50
theta_0=c(200,20)
c=0.1
postCov=diag(c(100,5))

# Defining function for sampling through metropolishastings
RVMSampler = function(previousVal, postCov, c, myFunction, ...) {
  proposalVal=rmvnorm(1, mean=previousVal, sigma=c*postCov)
  proposalVal[proposalVal<=0]=1e-6
  alpha=min(1, exp(myFunction(proposalVal,...)-myFunction(previousVal, ...)))
  u=runif(1)
  if(u < alpha) {
    return(list(theta=proposalVal, acceptProb=alpha))
  } else {
    return(list(theta=previousVal, acceptProb=alpha))
  }
}

logPrior = function(phi) {
  return(-2*log(phi))
}

logLike <- function(param, x){
  theta1 = param[1]
  theta2 = param[2]
  logPost = sum(logdNegBin(x, theta1, theta2))  - 2*log(theta2)
  return(logPost)
}

logPost = function(theta, data) {
  log_Prior=logPrior(theta[2])
  log_Like=logLike(theta, data)
  return(log_Prior+log_Like)
}

post_matrix = matrix(0, nIter+burnIn, 2)
# Setting initial values of beta to same initVals as in the optimizer (taken randomly from normal distrib)
post_matrix[1,]=theta_0
accProb=rep(0, nIter)
set.seed(12345)

for(i in 1:(nIter+burnIn)) {
  if(i<(nIter+burnIn)) {
    draw=RVMSampler(post_matrix[i,], postCov, c, logPost, data)
    post_matrix[i+1,]=draw$theta
    accProb[i+1]=draw$acceptProb
  }
}

iter=seq(1,nIter+burnIn,1)
plot(iter[-(1:burnIn)], post_matrix[-(1:burnIn),1], type="l", lwd=1, col="grey", main="Traceplot of mu in RVM",
     xlab=expression(mu), ylab="Value")
plot(iter[-(1:burnIn)], post_matrix[-(1:burnIn),2], type="l", lwd=1, col="grey", main="Traceplot of phi in RVM",
     xlab=expression(phi), ylab="Value")
mean(accProb)

## This MCMC sampler is not efficient since it moves very slowly and is therefore probably not exploring
## the whole posterior distribution.We can also see that the acceptance probability for this algorithm
## is around 84,4 % and it should be around 30 %. Once could tune the c param to lower the acceptance probability.
## One example is to increase c to a value of 3 which would yield in approximately 30 % acceptance rate. 

## b) Instead simulate from posterior using metropolis hastings. 

c=0.8

MHSampler = function(previousVal, postCov, c, myFunction, ...) {
  proposalVal_mu=rgamma(1, c*previousVal[1], c)
  proposalVal_phi=rgamma(1, c*previousVal[2], c)
  proposalVal=c(proposalVal_mu, proposalVal_phi)
  proposalVal[proposalVal<=0]=1e-6
  alpha=min(1, exp(myFunction(proposalVal,...)-myFunction(previousVal, ...)+
                     dgamma(previousVal[1], c*proposalVal[1], c)+dgamma(previousVal[2],c*proposalVal[2],c)-
                     dgamma(proposalVal[1], c*previousVal[1], c)-dgamma(proposalVal[2], c*proposalVal[2],c)))
  u=runif(1)
  if(u < alpha) {
    return(list(theta=proposalVal, acceptProb=alpha))
  } else {
    return(list(theta=previousVal, acceptProb=alpha))
  }
}

post_matrix2 = matrix(0, nIter+burnIn, 2)
theta_0=c(200,10)
post_matrix2[1,]=theta_0
accProb2=rep(0, nIter)
set.seed(12345)

for(i in 1:(nIter+burnIn)) {
  if(i<(nIter+burnIn)) {
    draw=MHSampler(post_matrix2[i,], postCov, c, logPost, data)
    post_matrix2[i+1,]=draw$theta
    accProb2[i+1]=draw$acceptProb
  }
}

plot(iter[-(1:burnIn)], post_matrix2[-(1:burnIn),1], type="l", lwd=1, col="grey", main="Traceplot of mu in MH",
     xlab=expression(mu), ylab="Value")
plot(iter[-(1:burnIn)], post_matrix2[-(1:burnIn),2], type="l", lwd=1, col="grey", main="Traceplot of phi in MH",
     xlab=expression(phi), ylab="Value")
mean(accProb2)

## The new algorithm seems to rapidly explore the posterior which is good. The acceptance probability is also lower
## around 30 % which also indicates that this algorithm is better than the previous one. 
```

#2019-10-31

##Assignment 1

```{r eval=FALSE}
## a) Theta is known

theta=0.6
eu_buy=0.6*30-0.4*10
eu_nobuy=0.6*90-120*0.4

## Answer: Should buy option

## c) Compute bayesian decision for day 101 based on information in b). 

theta_new=13/21
eu_buy_new=theta_new*30-(1-theta_new)*10
eu_nobuy_new=theta_new*90-(1-theta_new)*120

## Answer: Should buy since utility higher. 
```

##Assignment 2

```{r eval=FALSE}
## a) Consider poisson likelihood model. Use conjugate prior and plot posterior in given interval. 
## Compute posterior probability that theta is smaller than 21. 

# Calculations show that alpha=20, beta=1

data=Traffic$y
alpha=20
beta=1
n=length(data)

# We know that Poisson with gamma prior is gamma distributed with alphaNew=alpha+sum(data), betaNew=beta+n

grid=seq(18,24,0.01)
post_distrib=dgamma(grid, shape=alpha+sum(data), rate=beta+n)
plot(grid, post_distrib, type="l", lwd=2, main="Posterior distrib. of theta", xlab=expression(theta))
post_prob=pgamma(21, shape=alpha+sum(data), rate=beta+n)

## Answer: Probability is 0.0557

## b) Two independent poisson models. 

data_model1=Traffic[which(Traffic[,3]=="yes"),]$y
data_model2=Traffic[which(Traffic[,3]=="no"),]$y

alpha_1=20+sum(data_model1)
alpha_2=20+sum(data_model2)
beta_1=1+length(data_model1)
beta_2=1+length(data_model2)
post_distrib_1=rgamma(5000, shape=alpha_1, rate=beta_1)
post_distrib_2=rgamma(5000, shape=alpha_2, rate=beta_2)
hist(post_distrib_1, breaks=50)
hist(post_distrib_2, breaks=50)
post_diff=post_distrib_2-post_distrib_1
hist(post_diff,
     main="Posterior distribution of difference between no speedlimit and speedlimit", xlab="No. of accidents")
quantile(post_diff, prob=c(0.025, 0.975))
mean(post_diff)

## We can see that the difference between the two distributions is larger than 0 with high probability. In this
## case we can say that the difference in traffic accidents between when no speed limit were applied and 
## when a speed limit were applied is between 2.82 and 5.53 approximately with 95 % posterior probability. 
## The conclusion from this is that yes, a speed limit leads to a lower amount of accidents.

## c) A politician claims that the experiment proves that introducing speed limit decreases the number
## of accidents by at least 15 %. 

mean(0.85*post_distrib_2>post_distrib_1)

## Likely that the decrease yields 15 % but 86 % probable and not 95 % probability which is commonly used
## in statistical experiments. 

```

##Assignment 3

```{r eval=FALSE}
## c) Make simulations of joins posterior of v and pi using Gibbs sampling.

x=20
lambda=10
alpha=2
beta=2
nIter=2000
burnIn=500

results=matrix(0,burnIn+nIter,2)
initVal=lambda # Since lambda=30
results[1,1]=initVal
results[1,2]=rnorm(1)
for (i in 1:(nIter+burnIn-1)) {
  z=rpois(1, lambda*(1-results[i,2]))
  results[i+1,1]=z+x
  results[i+1,2]=rbeta(1, alpha+x, beta+results[i+1,1]-x)
}

grid=seq(burnIn+1, nIter+burnIn)
barplot(table(results[(burnIn+1):(nIter+burnIn),1]), main="Marginal posterior of nu", xlab=expression(nu))
hist(results[(burnIn+1):(nIter+burnIn),2], breaks=50, main="Marginal posterior of pi", xlab=expression(pi))
plot(grid, results[(burnIn+1):(nIter+burnIn),2],type="l")
plot(grid, results[(burnIn+1):(nIter+burnIn),1], type="l")

## Convergence seems good since markov chain is exploring full posterior and have good mixing. 
```

##Assignment 4

```{r eval=FALSE}
## a) Use supplied stan model to do Bayesian inference. Draw 2000 posterior samples and use 500 for burnin. 
## Produce figure with scatter plot, overlay curve for mean of posterior predictive distrib, in range [0,25]. 
## Also overlay curves 90 % equal tail interval for same posterior predictive distrib given values of x in range [0,25]

# Load data
cars = cars

library(rstan)
LinRegModel <- '
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma2;
}
model {
  sigma2 ~ scaled_inv_chi_square(5,10);
  for (n in 1:N)
    y[n] ~ normal(alpha + beta * x[n], sqrt(sigma2));
}
'
x=cars$speed
y=cars$dist
nIter=2000
burnIn=500
N=dim(cars)[1]
data=list(N=N,x=x,y=y)
fit=stan(model_code=LinRegModel, data=data, iter=nIter, warmup = 500, chains=1)
print(fit)
postDraws=extract(fit)
alpha_draws=postDraws$alpha
beta_draws=postDraws$beta
sigma_draws=postDraws$sigma2
xGrid=seq(0,25)
n=length(alpha_draws)
mean_credInt=matrix(0,length(xGrid),3)
count=1
for (i in 1:length(xGrid)) {
  ysim=rep(0,length(nIter-burnIn))
  ysim=alpha_draws+beta_draws*xGrid[i]+rnorm(nIter-burnin, mean=0, sd=sqrt(sigma_draws))
  mean_credInt[count,1]=mean(ysim)
  mean_credInt[count,-1]=quantile(ysim, probs = c(0.05, 0.95))
  count=count+1
}

plot(x,y,xlab="Speed", ylab="Distance", col="blue", main="Plot for model with constant sigma prior")
lines(xGrid, mean_credInt[,1], lwd=2, col="red")
lines(xGrid, mean_credInt[,2], lwd=1, lty=2)
lines(xGrid, mean_credInt[,3], lwd=1, lty=2)
legend("topleft", legend=c("Data", "Posterior mean", "90 % cred interval"), col=c("blue", "red", "grey"), 
       pch=c(1, NaN, NaN), lty=c(NaN, 1, 2), lwd=c(NaN, 2, 1))

## b) Compute 95 % equal tail credible interval for alpha. Give real-world interpret of the interval. 

quantile(alpha_draws, probs=c(0.025, 0.975))

## The interpretation of the credible interval for alpha is that if the car has no speed it travels a negative
## distance between -31 and 4.25 approximately with 95 % posterior probability. This is not realistic. To prevent this
## a prior can be set to alpha with a mean around zero which however would make the linear prediction worse. 
## One can also use the log Normal distribution for y to force it to have a value above zero. 

## c) Reproduce results in b) with heteroscadastic variance. 

LinRegModel_hetero <- '
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma2[N];
  real gamma;
  real phi;
}
model {
  for (n in 1:N)
    sigma2[n] ~ scaled_inv_chi_square(5,exp(gamma+phi*x[n]));
  for (n in 1:N)
    y[n] ~ normal(alpha + beta * x[n], sqrt(sigma2[n]));
}
'
data=list(N=N,x=x,y=y)
fit2=stan(model_code=LinRegModel_hetero, data=data, iter=nIter, warmup = 500, chains=1)
print(fit2)
postDraws2=extract(fit2)
alpha_draws=postDraws2$alpha
beta_draws=postDraws2$beta
sigma_draws=postDraws2$sigma2
xGrid=seq(0,25)
n=length(alpha_draws)
mean_credInt=matrix(0,length(xGrid),3)
count=1
for (i in 1:length(xGrid)) {
  rinv=rchisq(nIter-burnIn, 5)
  sigma_draw=5*exp(postDraws2$gamma + xgrid[i] * postDraws2$phi)^2/rinv
  ysim=rep(0,length(nIter-burnIn))
  ysim=alpha_draws+beta_draws*xGrid[i]+rnorm(nIter-burnin, mean=0, sd=sqrt(sigma_draw))
  mean_credInt[count,1]=mean(ysim)
  mean_credInt[count,-1]=quantile(ysim, probs = c(0.05, 0.95))
  count=count+1
}

plot(x,y,xlab="Speed", ylab="Distance", col="blue", main="Plot of model with heteroscadastic sigma prior")
lines(xGrid, mean_credInt[,1], lwd=2, col="red")
lines(xGrid, mean_credInt[,2], lwd=1, lty=2)
lines(xGrid, mean_credInt[,3], lwd=1, lty=2)
legend("topleft", legend=c("Data", "Posterior mean", "90 % cred interval"), col=c("blue", "red", "grey"), 
       pch=c(1, NaN, NaN), lty=c(NaN, 1, 2), lwd=c(NaN, 2, 1))

## The new model seems to capture the data better than the old one. 
```


