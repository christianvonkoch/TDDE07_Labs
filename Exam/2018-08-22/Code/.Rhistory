normalVar = (alpha + s - 1)*(beta + f - 1)/(alpha + beta + n - 2)^3
xgrid = seq(0,1,.001)
plot(xgrid,dbeta(xgrid,alpha+s,beta+f),type="l")
lines(xgrid,dnorm(xgrid,normalMean,sqrt(normalVar)),col=2)
legend("topright", box.lty = 1, legend = c("Beta posterior","Normal approx."),
col = c("black",'red'), lwd = 2)
##########
# Compare Normal approximation of Beta posterior to the true posterior (MathExercise 3)
#
# Author: Per Siden
# Date: 2018-04-11
##########
# 3c
alpha = 1
beta = 1
n = 6
s = 1
f = n-s
normalMean = (alpha + s - 1)/(alpha+beta+n-2)
normalVar = (alpha + s - 1)*(beta + f - 1)/(alpha + beta + n - 2)^3
xgrid = seq(0,1,.001)
plot(xgrid,dbeta(xgrid,alpha+s,beta+f),type="l")
lines(xgrid,dnorm(xgrid,normalMean,sqrt(normalVar)),col=2)
legend("topright", box.lty = 1, legend = c("Beta posterior","Normal approx."),
col = c("black",'red'), lwd = 2)
# 3d
alpha = 1
beta = 1
n = 12
s = 2
f = n-s
normalMean = (alpha + s - 1)/(alpha+beta+n-2)
normalVar = (alpha + s - 1)*(beta + f - 1)/(alpha + beta + n - 2)^3
xgrid = seq(0,1,.001)
plot(xgrid,dbeta(xgrid,alpha+s,beta+f),type="l")
lines(xgrid,dnorm(xgrid,normalMean,sqrt(normalVar)),col=2)
legend("topright", box.lty = 1, legend = c("Beta posterior","Normal approx."),
col = c("black",'red'), lwd = 2)
# 3d
alpha = 1
beta = 1
n = 120
s = 20
f = n-s
normalMean = (alpha + s - 1)/(alpha+beta+n-2)
normalVar = (alpha + s - 1)*(beta + f - 1)/(alpha + beta + n - 2)^3
xgrid = seq(0,1,.001)
plot(xgrid,dbeta(xgrid,alpha+s,beta+f),type="l")
lines(xgrid,dnorm(xgrid,normalMean,sqrt(normalVar)),col=2)
legend("topright", box.lty = 1, legend = c("Beta posterior","Normal approx."),
col = c("black",'red'), lwd = 2)
data <- read.table("TempLinkoping.txt",header=TRUE)
data2 <- cbind(data,time2=data$time^2)
sigma0 <- 5
nu0 <- 10
mu0 <- c(0,100,-100)
omega0 <- diag(c(5,2.5,5))
plot(mu0[1] + data2$time*mu0[2] + data2$time2*mu0[3],col=2,type="l",
xlab="day",ylab="Temperature",main="Mean prior regression curve")
library(mvtnorm)
set.seed(123)
N <- 100
rc <- rchisq(N,nu0)
sigmasim <- nu0*sigma0^2/rc
betasim <- matrix(0,N,3)
for(i in 1:N) {
betasim[i,] <- rmvnorm(1,mu0,sigmasim[i]*solve(omega0))
}
plot(betasim[1,1] + data2$time*betasim[1,2] + data2$time2*betasim[1,3],type="l",
ylim=c(-10,40),xlab="day",ylab="Temperature",main="100 simulations of the regression \n curve from the prior")
for(i in 2:N) {
lines(betasim[i,1] + data2$time*betasim[i,2] + data2$time2*betasim[i,3])
}
library(mvtnorm)
set.seed(123)
N <- 1000
n <- nrow(data2)
nun <- nu0 + n
X <- cbind(rep(1,n), data2$time, data2$time2)
y <- data2$temp
omegan <- t(X)%*%X + omega0
betahat <- solve(t(X)%*%X)%*%t(X)%*%y
mun <- solve(omegan)%*%(t(X)%*%X%*%betahat + omega0%*%mu0)
sigman <- c(sqrt((nu0*sigma0^2 + (t(y)%*%y) + t(mu0)%*%omega0%*%mu0 - t(mun)%*%omegan%*%mun)/nun))
rc <- rchisq(N,nun)
sigmasim <- nun*sigman^2/rc
betasim <- matrix(0,N,3)
for(i in 1:N) {
betasim[i,] <- rmvnorm(1,mun,sigmasim[i]*solve(omegan))
}
fmean = matrix(0,n,1)
fbands = matrix(0,n,2)
for(t in 1:n){
f = X[t,]%*%t(betasim)
fmean[t] = mean(f)
fbands[t,] = quantile(f,probs=c(.05,.95))
}
plot(data2$time,data2$temp,type='p',xlab="day",ylab="Temperature",main="Posterior mean and bands")
lines(data2$time,fmean)
lines(data2$time,fbands[,1],col=2)
lines(data2$time,fbands[,2],col=2)
colMeans(betasim)
mean(sigmasim)
mean(sigmasim^2)
?function
(j)
)
?function(j)
)
xtildesim = -betasim[,2]/(2*betasim[,3])
hist(xtildesim,20,main="Posterior distribution of x tilde",xlab="x tilde")
WomenWork <- read.table("WomenWork.dat",header=TRUE)
glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)
library(mvtnorm)
y <- as.vector(WomenWork[,1])
X <- as.matrix(WomenWork[,2:9])
covNames <- names(WomenWork)[2:length(names(WomenWork))]
nPara <- dim(X)[2]
# Setting up the prior
tau <- 10
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara)
# Defining the functions that returns the log posterior (Logistic and Probit models). Note that the first input argument of
# this function must be the one that we optimize on, i.e. the regression coefficients.
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect)
linPred <- X%*%betaVect
# evaluating the log-likelihood
logLik <- sum( linPred*y -log(1 + exp(linPred)))
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
# evaluating the prior
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE)
# add the log prior and log-likelihood together to get log posterior
return(logLik + logPrior)
}
# Calling the optimization routine Optim. Note the auxilliary arguments that are passed to the function logPost
# Note how I pass all other arguments of the function logPost (i.e. all arguments except betaVect which is the one that we are trying to optimize over) to the R optimizer.
# The argument control is a list of options to the optimizer. Here I am telling the optimizer to multiply the objective function (i.e. logPost) by -1. This is because
# Optim finds a minimum, and I want to find a maximum. By reversing the sign of logPost I can use Optim for my maximization problem.
# Different starting values. Ideally, any random starting value gives you the same optimum (i.e. optimum is unique)
# initVal <- as.vector(rep(0,dim(X)[2]));
# Or a random starting vector: as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS
initVal = glmModel$coefficients
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Printing the results to the screen
postMode <- OptimResults$par
postCov <- -solve(OptimResults$hessian) # Posterior covariance matrix is -inv(Hessian)
names(postMode) <- covNames # Naming the coefficient by covariates
# approxPostStd <- sqrt(diag(postCov)) # Computing approximate standard deviations.
# names(approxPostStd) <- covNames # Naming the coefficient by covariates
print('The posterior mode is:')
print(postMode)
print('The posterior covariance matrix is:')
print(postCov)
print('A 95% credible interval for  NSmallChild is')
print(postMode[7] + 1.96*sqrt(postCov[7,7])*c(-1,1))
simuPredPost <- function(Ns,postMode,postCov,X){
X = t(as.matrix(X))
n <- dim(X)[1]
nPara <- length(postMode)
betaSim = matrix(0,Ns,nPara)
for(i in 1:Ns) {
betaSim[i,] <- rmvnorm(1,postMode,postCov)
}
linPred <- X%*%t(betaSim)
ySim <- matrix(runif(n*Ns),n,Ns) > (exp(linPred) / (1+exp(linPred)))
return(as.numeric(ySim))
}
X = c(1,10,8,10,1,40,1,1)
ySim = simuPredPost(1000,postMode,postCov,X)
hist(ySim,xlab="y",main="Predictive distribution")
mean(ySim)
AR1Sim <- function(T,mu,phi,sigma2){
x = rep(0,T)
x[1] = mu
for(t in 2:T) {
x[t] = mu + phi*(x[t-1]-mu) + sqrt(sigma2)*rnorm(1)
}
x
}
set.seed(1)
T = 200
mu = 10
sigma2 = 2
plot(AR1Sim(T,mu,.95,sigma2),type="l",xlab="time",ylab="",main="AR(1) realisations")
lines(AR1Sim(T,mu,.3,sigma2),col=2)
set.seed(1)
y = AR1Sim(T,mu,.95,sigma2)
x = AR1Sim(T,mu,.3,sigma2)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
ARmodel <- '
data {
int<lower=0> T;
vector[T] x;
}
parameters {
real mu;
real phi;
real<lower=0> sigma2;
}
model {
mu ~ normal(0,100);
sigma2 ~ scaled_inv_chi_square(.001,2);
phi ~ normal(0,1);
for (t in 2:T)
x[t] ~ normal(mu + phi*(x[t-1] - mu),sqrt(sigma2));
}
'
data <- list(T = T, x=x)
# Fit the model
fitx <- stan(model_code=ARmodel,
data=data,
warmup=1000,
iter=2000,
chains=4)
simuPredPost <- function(Ns,postMode,postCov,X){
X = t(as.matrix(X))
n <- dim(X)[1]
nPara <- length(postMode)
betaSim = matrix(0,Ns,nPara)
for(i in 1:Ns) {
betaSim[i,] <- rmvnorm(1,postMode,postCov)
}
linPred <- X%*%t(betaSim)
ySim <- matrix(runif(n*Ns),n,Ns) < (exp(linPred) / (1+exp(linPred)))
return(as.numeric(ySim))
}
X = c(1,10,8,10,1,40,1,1)
ySim = simuPredPost(1000,postMode,postCov,X)
hist(ySim,xlab="y",main="Predictive distribution")
mean(ySim)
WomenWork <- read.table("WomenWork.dat",header=TRUE)
glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)
glmModel
summary(glmModel)
diag(1,7,7)
12^7
WomenWork <- read.table("WomenWork.dat",header=TRUE)
glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)
summary(glmModel)
?rchisq
?sapply
sapply(rep(0,10),function(beta) {rnorm(0,1)})
hej = sapply(rep(0,10),function(beta) {rnorm(0,1)})
hej = cbind(sapply(rep(0,10),function(beta) {rnorm(0,1)}))
he
hej
hej = cbind(t(sapply(rep(0,10),function(beta) {rnorm(0,1)})))
hej
hej = cbind(t(sapply(rep(0,10),function(beta) {rnorm(1,0,1)})))
hej
?pnorm
qnorm(.95)
WomenWork <- read.table("WomenWork.dat",header=TRUE)
glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)
summary(glmModel)
library(mvtnorm)
y <- as.vector(WomenWork[,1])
X <- as.matrix(WomenWork[,2:9])
covNames <- names(WomenWork)[2:length(names(WomenWork))]
nPara <- dim(X)[2]
# Setting up the prior
tau <- 10
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara)
# Defining the functions that returns the log posterior (Logistic and Probit models). Note that the first input argument of
# this function must be the one that we optimize on, i.e. the regression coefficients.
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect)
linPred <- X%*%betaVect
# evaluating the log-likelihood
logLik <- sum( linPred*y -log(1 + exp(linPred)))
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
# evaluating the prior
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE)
# add the log prior and log-likelihood together to get log posterior
return(logLik + logPrior)
}
# Calling the optimization routine Optim. Note the auxilliary arguments that are passed to the function logPost
# Note how I pass all other arguments of the function logPost (i.e. all arguments except betaVect which is the one that we are trying to optimize over) to the R optimizer.
# The argument control is a list of options to the optimizer. Here I am telling the optimizer to multiply the objective function (i.e. logPost) by -1. This is because
# Optim finds a minimum, and I want to find a maximum. By reversing the sign of logPost I can use Optim for my maximization problem.
# Different starting values. Ideally, any random starting value gives you the same optimum (i.e. optimum is unique)
# initVal <- as.vector(rep(0,dim(X)[2]));
# Or a random starting vector: as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS
initVal = glmModel$coefficients
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Printing the results to the screen
postMode <- OptimResults$par
postCov <- -solve(OptimResults$hessian) # Posterior covariance matrix is -inv(Hessian)
names(postMode) <- covNames # Naming the coefficient by covariates
# approxPostStd <- sqrt(diag(postCov)) # Computing approximate standard deviations.
# names(approxPostStd) <- covNames # Naming the coefficient by covariates
print('The posterior mode is:')
print(postMode)
print('The posterior covariance matrix is:')
print(postCov)
print('A 95% credible interval for  NSmallChild is')
print(postMode[7] + 1.96*sqrt(postCov[7,7])*c(-1,1))
sqrt(postCov[7,7])
postCov[7,7]
postMode[7]
# Print the fitted model
print(fit,digits_summary=3)
source('~/Dropbox/phd/Undervisning/BLprivate/LabSolutions/RStanExamTest.R')
source('~/Dropbox/phd/Undervisning/BLprivate/LabSolutions/RStanExamTest.R')
source('~/Dropbox/phd/Undervisning/BLprivate/LabSolutions/RStanExamTest.R')
source('~/Dropbox/phd/Undervisning/BLprivate/LabSolutions/RStanExamTest.R')
source('~/Dropbox/phd/Undervisning/BLprivate/LabSolutions/RStanExamTest.R')
source('~/Dropbox/phd/Undervisning/BLprivate/LabSolutions/RStanExamTest.R')
source('~/Dropbox/phd/Undervisning/BLprivate/LabSolutions/RStanExamTest.R')
library(coda)
data <- read.table("rainfall.dat",header=TRUE)
y = data$X136
set.seed(123)
burnin <- 1000
niter <- 4000
mu0 <- 20
tau0 <- 50
sigma0 <- 100
nu0 <- 10
# start values
sigma <- 100
mu <- 20
mu_vec <- rep(0,burnin+niter)
sigma_vec <- rep(0,burnin+niter)
mu_vec[1] <- mu
sigma_vec[1] <- sigma
ybar <- mean(y)
n <- length(y)
nun <- nu0 + n
for(i in 2:(burnin+niter)){
w <- n/sigma^2 / (n/sigma^2 + 1/tau0^2)
mun <- w*ybar + (1-w)*mu0
taun <- sqrt(sigma^2*tau0^2 / (n*tau0^2 + sigma^2))
mu <- rnorm(1,mun,taun)
mu_vec[i] <- mu
sigman2 <- (nu0*sigma0^2+sum((y-mu)^2)) / nun
rc <- rchisq(1,nun)
sigma <- sqrt(nun*sigman2/rc)
sigma_vec[i] <- sigma
}
par(mfrow=c(2,1))
plot(mu_vec[(burnin+1):(burnin+niter)],type="l",main="Simulation of mu",xlab="iteration",
ylab="mu")
plot(sigma_vec[(burnin+1):(burnin+niter)],type="l",main="Simulation of sigma",xlab="iteration",
ylab="sigma")
# mu_acf <- acf(mu_vec[(burnin+1):(burnin+niter)],plot=FALSE)
# mu_IF <- 1 + 2*sum(mu_acf$acf[-1])
# sigma_acf <- acf(sigma_vec[(burnin+1):(burnin+niter)],plot=FALSE)
# sigma_IF <- 1 + 2*sum(sigma_acf$acf[-1])
mu_IF = niter / effectiveSize(mu_vec[(burnin+1):(burnin+niter)])
sigma_IF = niter / effectiveSize(sigma_vec[(burnin+1):(burnin+niter)])
est <- c(mean(mu_vec),mean(sigma_vec))
std <- c(sd(mu_vec),sd(sigma_vec))
IF <- c(mu_IF,sigma_IF)
names(est) <- c("mu","sigma")
print(cbind(est,std,IF))
source('~/Dropbox/phd/Undervisning/BLprivate/LabSolutions/NormalMixtureGibbsHelpers.R')
x = as.matrix(as.numeric(y))
nComp = 2
# Prior options
alpha <- 1*rep(1,nComp) # Dirichlet(alpha)
muPrior <- rep(20,nComp) # Prior mean of theta
tau2Prior <- rep(50,nComp) # Prior std theta
sigma2_0 <- rep(var(x),nComp) # s20 (best guess of sigma2)
nu0 <- rep(10,nComp) # degrees of freedom for prior on sigma2
# MCMC options
nIter <- 1100 # Number of Gibbs sampling draws
# Plotting options
plotFit <- FALSE
GibbsOutput = sampleMixtureByGibbs(x,nComp,alpha,muPrior,tau2Prior,sigma2_0,nu0,nIter,plotFit)
burnin = 100
par(mfrow=c(3,2))
plot(GibbsOutput$thetaMat[(burnin+1):nIter,1],type="l",main="Simulation of theta1",ylab="theta1")
plot(GibbsOutput$thetaMat[(burnin+1):nIter,2],type="l",main="Simulation of theta2",ylab="theta2")
plot(GibbsOutput$sigma2Mat[(burnin+1):nIter,1],type="l",main="Simulation of sigma2_1",ylab="sigma2_1")
plot(GibbsOutput$sigma2Mat[(burnin+1):nIter,2],type="l",main="Simulation of sigma2_2",ylab="sigma2_2")
plot(GibbsOutput$wMat[(burnin+1):nIter,1],type="l",main="Simulation of w1",ylab="w1")
plot(GibbsOutput$wMat[(burnin+1):nIter,2],type="l",main="Simulation of w2",ylab="w2")
theta = colMeans(GibbsOutput$thetaMat[(burnin+1):nIter,])
sigma2 = colMeans(GibbsOutput$sigma2Mat[(burnin+1):nIter,])
w = colMeans(GibbsOutput$wMat[(burnin+1):nIter,])
normalColor <- c("magenta")
lineColors <- c("blue", "green")
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
ylim <- c(0,2*max(hist(x,plot=FALSE)$density))
hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = "Densities", ylim = ylim)
lines(xGrid, dnorm(xGrid,est[1],sd = est[2]), type = "l", lwd = 2, col = normalColor)
mixDens <- rep(0,length(xGrid))
components <- c()
for (j in 1:nComp){
compDens <- dnorm(xGrid,theta[j],sd = sqrt(sigma2[j]))
mixDens <- mixDens + w[j]*compDens
lines(xGrid, compDens, type = "l", lwd = 2, col = lineColors[j])
components[j] <- paste("Component ",j)
}
mixDensMean <- ((effIterCount-1)*mixDensMean + mixDens)/effIterCount
lines(xGrid, mixDens, type = "l", lty = 2, lwd = 3, col = 'red')
legend("topright", box.lty = 1, legend = c("Data histogram","Normal",components, 'Mixture'),
col = c("black",normalColor,lineColors[1:nComp], 'red'), lwd = 2)
sqrt(0.1)
.2*.2
AR1Sim <- function(T,mu,phi,sigma2){
x = rep(0,T)
x[1] = mu
for(t in 2:T) {
x[t] = mu + phi*(x[t-1]-mu) + sqrt(sigma2)*rnorm(1)
}
x
}
set.seed(1)
T = 200
mu = 10
sigma2 = 2
plot(AR1Sim(T,mu,.95,sigma2),type="l",xlab="time",ylab="",main="AR(1) realisations")
lines(AR1Sim(T,mu,.3,sigma2),col=2)
set.seed(1)
y = AR1Sim(T,mu,.95,sigma2)
x = AR1Sim(T,mu,.3,sigma2)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
data <- read.table("campy.dat",header=TRUE)
c = as.numeric(as.vector(data$c))
T = length(c)
P = 1
# Model
LIMmodel <- '
data {
int<lower=0> T;
int<lower=0> y[T];
int<lower=0> P;
}
parameters {
real mu;
real phi[P];
real<lower=0> sigma2;
vector[T] x;
}
model {
mu ~ normal(0,100);
sigma2 ~ scaled_inv_chi_square(.001,2);
phi ~ normal(0,1);
// sigma2 ~ scaled_inv_chi_square(100,.1);
for (t in (P+1):T){
real ARmean = mu;
for (p in 1:P){
ARmean += phi[p] * (x[t-p] - mu);
}
x[t] ~ normal(ARmean,sqrt(sigma2));
}
for (t in 1:T)
y[t] ~ poisson(exp(x[t]));
}
'
# Fit the model
warmup=1000
iter=2000
niter = 4*(iter-warmup)
data <- list(T=T, y=c,P=P)
fit <- stan(model_code=LIMmodel,
data=data,
warmup=warmup,
iter=iter,
chains=4)
summary(fit)
summary(fit)$summary
1/(1/2+1/3)
.5/(1/2+1/3)
2/(1/2+1/3)
2/(1/1+1/3)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
2/(1/0.1+1/.5)
100^(1/3)
setwd("~/Dropbox/phd/Undervisning/BLprivate/Exams2018/20180822/Code")
source('~/Dropbox/phd/Undervisning/BLprivate/Exams2018/20180822/Code/Exam732A91_180822_Sol.R')
n = 3
xbar = 5
a = .5
b = .5
logmarglik1 <- lbeta(n+a,n*xbar+b) - lbeta(a,b)
lbeta(a,b)
lbeta(n+a,n*xbar+b)
logmarglik2 <- n*log(.5) + n*xbar*log(.5)
unnorm_modelprob = c(exp(logmarglik1)/10,exp(logmarglik2)*9/10)
modelprob = unnorm_modelprob / sum(unnorm_modelprob)
modelprob
logmarglik1
logmarglik2
exp(-12)
.5^n*.5^(n*xbar)
log(.5^n*.5^(n*xbar))
