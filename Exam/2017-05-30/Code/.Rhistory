# 3a iii)
quantile(Results$sigma2Sample, c(0.025, 0.975))
quantile(Results$betaSample[,1], c(0.025, 0.975))
quantile(Results$betaSample[,2], c(0.025, 0.975))
quantile(Results$betaSample[,3], c(0.025, 0.975))
quantile(Results$betaSample[,4], c(0.025, 0.975))
# 3b
hist(Results$betaSample[,4]-Results$betaSample[,3],50)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Exam/2017-05-30/2017-05-30_Assignment1.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Exam/2017-05-30/2017-05-30_Assignment3.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Exam/2017-05-30/2017-05-30_Assignment3.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Exam/2017-05-30/Code/ExamData.R', echo=TRUE)
# Installing and loading a package with the multivariate normal distribution
#install.packages("mvtnorm")
library(mvtnorm)
# Loading functions supplied in the ExamData.R file
# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
return((df*scale)/rchisq(n,df=df))
}
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
# Reading data
load("cars.RData")
y = cars$mpg
X = as.matrix(cars[,2:5])
nCovs = dim(X)[2]
# Prior
mu_0 = rep(0,nCovs)
Omega_0 = 0.01*diag(nCovs)
v_0 = 1
sigma2_0 = 36
# 3a i)
nIter = 1000
Results = BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
hist(Results$sigma2Sample, 50, freq=FALSE, main = 'Posterior of sigma2', xlab = expression(sigma^2), )
hist(Results$betaSample[,1],50, freq=FALSE, main = 'Posterior of beta0', xlab = expression(beta_0), )
hist(Results$betaSample[,2], 50, freq=FALSE, main = 'Posterior of beta_1', xlab = expression(beta_1), )
hist(Results$betaSample[,3], 50, freq=FALSE, main = 'Posterior of beta_2', xlab = expression(beta_2), )
hist(Results$betaSample[,4], 50, freq=FALSE, main = 'Posterior of beta_3', xlab = expression(beta_3), )
# 3a ii)
# The posterior median is the optimal estimator under linear loss function (see Slides Lecture 4)
median(Results$betaSample[,1])
median(Results$betaSample[,2])
median(Results$betaSample[,3])
median(Results$betaSample[,4])
# 3a iii)
quantile(Results$sigma2Sample, c(0.025, 0.975))
quantile(Results$betaSample[,1], c(0.025, 0.975))
quantile(Results$betaSample[,2], c(0.025, 0.975))
quantile(Results$betaSample[,3], c(0.025, 0.975))
quantile(Results$betaSample[,4], c(0.025, 0.975))
# Interpretation of probability interval beta_1 in [-4.764942 -1.593364]
# A one unit increase in weight lowers the miles per gallon by between -4.764942 and -1.593364 miles
# with 95% (0.95) posterior probability.
# 3b)
# Here we want to know the posterior distribution of beta_3 == beta_4 or not.
# A natural way to investigate  this is compute the posterior distribution of
# theta = beta_3-beta_4 and to see if a 95% probability interval for theta
# covers the value theta=0.
# 3b
hist(Results$betaSample[,4]-Results$betaSample[,3],50)
quantile(Results$betaSample[,4]-Results$betaSample[,3],c(0.25,0.975))
# The distribution of theta gives quite high density around theta=0.
# The 95% interval covers theta=0
# We can not rule out the the effect from 6 and 8 cylinders are the same.
# 3c
ytilde <- rep(0,nIter)
xtilde <- c(1,3.5,0,0)
for (i in 1:nIter){
ytilde[i] = sum(xtilde*Results$betaSample[i,]) + rnorm(n=1, mean = 0, sd = sqrt(Results$sigma2Sample[i]))
}
hist(ytilde,40, main ="Predictive distribution of y", freq = FALSE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Exam/2017-05-30/2017-05-30_Assignment3.R', echo=TRUE)
# average all the resulting distributions.
nDraws = 1000
thetaDraws = rgamma(n = nDraws, shape = alphaGamma + sum(bids), rate = betaGamma + n)
PoisDensMean <- rep(0, length(xGrid))
for (i in 1:nDraws){
PoisDensMean = PoisDensMean + dpois(xGrid, lambda = thetaDraws[i])
}
PoisDensMean = PoisDensMean/nDraws # Average
lines(xGrid, PoisDensMean, type = "o", lwd = 1, col = "blue", pch = 'o', cex = 0.6)
legend(x = 6 , y = 0.2, legend = c("Data","Poisson (mean theta)",
"Poisson (mean density)"), col = c("black","red","blue"),
lty = c(1,1,1), lwd = c(3,3,1), cex = 0.8)
# As you can see in the Figure, i) and ii) gives very very similar distributions for the data.
# You can also see that the Poisson model is terrible for this data sets. Very poor fit.
# 2c
# Here is the code that was supplied in the ExamData.R file:
# Code for Problem 3 - Exam in Bayesian Learning 2017-05-30
GibbsMixPois <- function(x, nComp, alpha, alphaGamma, betaGamma, xGrid, nIter){
# Gibbs sampling for a mixture of Poissons
# Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
#
# INPUTS:
#   x - vector with data observations (counts)
#   nComp - Number of mixture components to be fitted
#   alpha - The prior on the mixture component weights is w ~ Dir(alpha, alpha,..., alpha)
#   alphaGamma and betaGamma -
#              The prior on the mean (theta) of the Poisson mixture components is
#              theta ~ Gamma(alphaGamma, betaGamma) [rate parametrization of the Gamma dist]
#   xGrid - the grid of data values over which the mixture is evaluated and plotted
#   nIter - Number of Gibbs iterations
#
# OUTPUTS:
#   results$wSample     - Gibbs sample of mixture component weights. nIter-by-nComp matrix
#   results$thetaSample - Gibbs sample of mixture component means.   nIter-by-nComp matrix
#   results$mixDensMean - Posterior mean of the estimated mixture density over xGrid.
####### Defining a function that simulates from a Dirichlet distribution
rDirichlet <- function(param){
nCat <- length(param)
thetaDraws <- matrix(NA,nCat,1)
for (j in 1:nCat){
thetaDraws[j] <- rgamma(1,param[j],1)
}
thetaDraws = thetaDraws/sum(thetaDraws) # Diving every column of ThetaDraws by the sum of the elements in that column.
return(thetaDraws)
}
# Simple function that converts between two different representations of the mixture allocation
S2alloc <- function(S){
n <- dim(S)[1]
alloc <- rep(0,n)
for (i in 1:n){
alloc[i] <- which(S[i,] == 1)
}
return(alloc)
}
# Initial values for the Gibbs sampling
nObs <- length(x)
S <- t(rmultinom(nObs, size = 1 , prob = rep(1/nComp,nComp))) # nObs-by-nComp matrix with component allocations.
theta <- rep(mean(x), nComp) # Each component is initialized at the mean of the data
# Setting up the grid where the mixture density is evaluated.
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
# Setting up matrices to store the draws
wSample <- matrix(0, nIter, nComp)
thetaSample <- matrix(0, nIter, nComp)
probObsInComp <- rep(NA, nComp)
# Setting up the priors - the same prior for all components
alpha <- rep(alpha, nComp)
alphaGamma <- rep(alphaGamma, nComp)
betaGamma <- rep(betaGamma, nComp)
# HERE STARTS THE ACTUAL GIBBS SAMPLING
for (k in 1:nIter){
# message(paste('Iteration number:',k)) # uncomment this is you want print the iterations
alloc <- S2alloc(S) # Function that converts between different representations of the group allocations
nAlloc <- colSums(S)
# Step 1 - Update components probabilities
w <- rDirichlet(alpha + nAlloc)
wSample[k,] <- w
# Step 2 - Update theta's in Poisson components
for (j in 1:nComp){
theta[j] <- rgamma(1, shape = alphaGamma + sum(x[alloc == j]), rate = betaGamma + nAlloc[j])
}
thetaSample[k,] <- theta
# Step 3 - Update allocation
for (i in 1:nObs){
for (j in 1:nComp){
probObsInComp[j] <- w[j]*dpois(x[i], lambda = theta[j])
}
S[i,] <- t(rmultinom(1, size = 1 , prob = probObsInComp/sum(probObsInComp)))
}
# Computing the mixture density at the current parameters, and averaging that over draws.
effIterCount <- effIterCount + 1
mixDens <- rep(0,length(xGrid))
for (j in 1:nComp){
compDens <- dpois(xGrid, lambda = theta[j])
mixDens <- mixDens + w[j]*compDens
}
mixDensMean <- ((effIterCount-1)*mixDensMean + mixDens)/effIterCount
}
return(results = list(wSample = wSample, thetaSample = thetaSample, mixDensMean = mixDensMean))
}
# We now use this code to simulate from GibbsMixPois.R with K=2
GibbsResults2 <- GibbsMixPois(x = bids, nComp = 2, alpha = 1, alphaGamma = 1, betaGamma = 1,
xGrid = xGrid, nIter = 500)
# And for K=3:
GibbsResults3 <- GibbsMixPois(x = bids, nComp = 3, alpha = 1, alphaGamma = 1, betaGamma = 1,
xGrid = xGrid, nIter = 500)
# 2d
# Plotting the data distribution against the Poisson and the fitted mixture of Poissons
mixDistr2 <- GibbsResults2$mixDensMean
mixDistr3 <- GibbsResults3$mixDensMean
lines(xGrid, mixDistr2, type = "o", lwd = 3, col = "purple", pch = 'o', cex = 0.6)
lines(xGrid, mixDistr3, type = "o", lwd = 3, col = "green", pch = 'o', cex = 0.6)
legend(x = 6 , y = 0.2, legend = c("Data","Poisson (mean theta)",
"Poisson (mean density)", "Mixture K=2", "Mixture K=3"),
col = c("black","red","blue","purple","green"),
lty = 1 , lwd = c(3,3,1,3,3), cex = .8)
# The two mixture distributions gives very similar fits, and much closer to the data distribution.
# Since K=2 gives similar fit to K=3, I would choose the K=2 model since it is simpler.
# 2e) See paper solution
###############################
########## Problem 3 ##########
###############################
# Installing and loading a package with the multivariate normal distribution
#install.packages("mvtnorm")
library(mvtnorm)
# Loading functions supplied in the ExamData.R file
# Defining a function that simulates from the scaled inverse Chi-square distribution
rScaledInvChi2 <- function(n, df, scale){
return((df*scale)/rchisq(n,df=df))
}
BayesLinReg <- function(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter){
# Direct sampling from a Gaussian linear regression with conjugate prior:
#
# beta | sigma2 ~ N(mu_0, sigma2*inv(Omega_0))
# sigma2 ~ Inv-Chi2(v_0,sigma2_0)
#
# Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com
#
# INPUTS:
#   y - n-by-1 vector with response data observations
#   X - n-by-nCovs matrix with covariates, first column should be ones if you want an intercept.
#   mu_0 - prior mean for beta
#   Omega_0  - prior precision matrix for beta
#   v_0      - degrees of freedom in the prior for sigma2
#   sigma2_0 - location ("best guess") in the prior for sigma2
#   nIter - Number of samples from the posterior (iterations)
#
# OUTPUTS:
#   results$betaSample     - Posterior sample of beta.     nIter-by-nCovs matrix
#   results$sigma2Sample   - Posterior sample of sigma2.   nIter-by-1 vector
# Compute posterior hyperparameters
n = length(y) # Number of observations
nCovs = dim(X)[2] # Number of covariates
XX = t(X)%*%X
betaHat <- solve(XX,t(X)%*%y)
Omega_n = XX + Omega_0
mu_n = solve(Omega_n,XX%*%betaHat+Omega_0%*%mu_0)
v_n = v_0 + n
sigma2_n = as.numeric((v_0*sigma2_0 + ( t(y)%*%y + t(mu_0)%*%Omega_0%*%mu_0 - t(mu_n)%*%Omega_n%*%mu_n))/v_n)
invOmega_n = solve(Omega_n)
# The actual sampling
sigma2Sample = rep(NA, nIter)
betaSample = matrix(NA, nIter, nCovs)
for (i in 1:nIter){
# Simulate from p(sigma2 | y, X)
sigma2 = rScaledInvChi2(n=1, df = v_n, scale = sigma2_n)
sigma2Sample[i] = sigma2
# Simulate from p(beta | sigma2, y, X)
beta_ = rmvnorm(n=1, mean = mu_n, sigma = sigma2*invOmega_n)
betaSample[i,] = beta_
}
return(results = list(sigma2Sample = sigma2Sample, betaSample=betaSample))
}
# Reading data
load("cars.RData")
y = cars$mpg
X = as.matrix(cars[,2:5])
nCovs = dim(X)[2]
# Prior
mu_0 = rep(0,nCovs)
Omega_0 = 0.01*diag(nCovs)
v_0 = 1
sigma2_0 = 36
# 3a i)
nIter = 1000
Results = BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
hist(Results$sigma2Sample, 50, freq=FALSE, main = 'Posterior of sigma2', xlab = expression(sigma^2), )
hist(Results$betaSample[,1],50, freq=FALSE, main = 'Posterior of beta0', xlab = expression(beta_0), )
hist(Results$betaSample[,2], 50, freq=FALSE, main = 'Posterior of beta_1', xlab = expression(beta_1), )
hist(Results$betaSample[,3], 50, freq=FALSE, main = 'Posterior of beta_2', xlab = expression(beta_2), )
hist(Results$betaSample[,4], 50, freq=FALSE, main = 'Posterior of beta_3', xlab = expression(beta_3), )
# 3a ii)
# The posterior median is the optimal estimator under linear loss function (see Slides Lecture 4)
median(Results$betaSample[,1])
median(Results$betaSample[,2])
median(Results$betaSample[,3])
median(Results$betaSample[,4])
# 3a iii)
quantile(Results$sigma2Sample, c(0.025, 0.975))
quantile(Results$betaSample[,1], c(0.025, 0.975))
quantile(Results$betaSample[,2], c(0.025, 0.975))
quantile(Results$betaSample[,3], c(0.025, 0.975))
quantile(Results$betaSample[,4], c(0.025, 0.975))
# Interpretation of probability interval beta_1 in [-4.764942 -1.593364]
# A one unit increase in weight lowers the miles per gallon by between -4.764942 and -1.593364 miles
# with 95% (0.95) posterior probability.
# 3b)
# Here we want to know the posterior distribution of beta_3 == beta_4 or not.
# A natural way to investigate  this is compute the posterior distribution of
# theta = beta_3-beta_4 and to see if a 95% probability interval for theta
# covers the value theta=0.
# 3b
hist(Results$betaSample[,4]-Results$betaSample[,3],50)
quantile(Results$betaSample[,4]-Results$betaSample[,3],c(0.25,0.975))
# The distribution of theta gives quite high density around theta=0.
# The 95% interval covers theta=0
# We can not rule out the the effect from 6 and 8 cylinders are the same.
# 3c
ytilde <- rep(0,nIter)
xtilde <- c(1,3.5,0,0)
for (i in 1:nIter){
ytilde[i] = sum(xtilde*Results$betaSample[i,]) + rnorm(n=1, mean = 0, sd = sqrt(Results$sigma2Sample[i]))
}
hist(ytilde,40, main ="Predictive distribution of y", freq = FALSE)
###############################
########## Problem 4 ##########
###############################
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Exam/2017-05-30/2017-05-30_Assignment3.R', echo=TRUE)
hist(pred_y, breaks=40, freq=FALSE)
new_x=c(1,3.5,0,0)
pred_y=rep(0,nIter)
for (i in 1:nIter) {
pred_y[i]=sum(new_x*jointPostDistrib$betaSample[i,])+rnorm(1,sd=sqrt(jointPostDistrib$sigma2Sample[i]))
}
hist(pred_y, breaks=40, freq=FALSE)
plot(post_dens(seq(0,10,1)))
post_dens = function(x) {
return(Gamma(6+x)/Gamma(13+x))
}
plot(post_dens(seq(0,10,1)))
help(gamma)
post_dens = function(x) {
return(gamma(6+x)/gamma(13+x))
}
plot(post_dens(seq(0,10,1)))
plot(post_dens(seq(0,10,1)), type="l")
exp_util=0
for (k in 1:10) {
exp_util=exp_util+(2^k-3)*post_dens(k)
}
print(exp_util)
posterior_prob=post_dens(seq(1,10))
posterior_prob=posterior_prob/sum(posterior_prob)
for (k in 1:10) {
exp_util=exp_util+(2^k-3)*post_dens(k)
}
print(exp_util)
# Let's start by truncating at k = 10
kMax = 10
post = rep(NA,kMax+1)
utility = rep(NA,kMax+1)
for (k in 0:kMax){
post[k+1] <- gamma(k + sumX + beta_)/gamma(k + sumX + n + alpha_ + beta_ +1)
utility[k+1] <- (2^k-1)-2
}
post <- post/sum(post)
barplot(post, names.arg = 0:kMax, main = "Predictive distribution", xlab = "x_(n+1)", ylab = "Probability")
x = c(0,2,0,3,0)
# First observation is zero since it took no loss before the first win,
# Second data point is 2 since there are two lossed before the second win and so on
n = length(x)
alpha_ = 1
beta_ = 1
sumX = sum(x)
# We need to decide an upper bound for k (we can't compute expected values by summing over an infinite set of number)
# Let's start by truncating at k = 10
kMax = 10
post = rep(NA,kMax+1)
utility = rep(NA,kMax+1)
for (k in 0:kMax){
post[k+1] <- gamma(k + sumX + beta_)/gamma(k + sumX + n + alpha_ + beta_ +1)
utility[k+1] <- (2^k-1)-2
}
post <- post/sum(post)
barplot(post, names.arg = 0:kMax, main = "Predictive distribution", xlab = "x_(n+1)", ylab = "Probability")
sum(utility*post) # = 7.416 which is larger than for the NoPlay action where the utility is 0. Play!
# x6=10 seems to yield a low enough probability to be an upper bound for sum
exp_util=0
posterior_prob=post_dens(seq(1,10))
posterior_prob=posterior_prob/sum(posterior_prob)
for (k in 1:11) {
exp_util=exp_util+(2^k-3)*post_dens(k)
}
print(exp_util)
posterior_prob=post_dens(seq(1,10))
exp_util=0
posterior_prob=post_dens(seq(1,11))
posterior_prob=posterior_prob/sum(posterior_prob)
for (k in 1:11) {
exp_util=exp_util+(2^k-3)*post_dens(k)
}
print(exp_util)
###############################
########## Problem 4 ##########
###############################
# 4 a) See paper.
# 4 b) See paper.
# 4 c) See also paper.
# The data W,L,L,W,W,L,L,L,W,W are the same as the following geometric data points:
x = c(0,2,0,3,0)
# First observation is zero since it took no loss before the first win,
# Second data point is 2 since there are two lossed before the second win and so on
n = length(x)
alpha_ = 1
beta_ = 1
sumX = sum(x)
# We need to decide an upper bound for k (we can't compute expected values by summing over an infinite set of number)
# Let's start by truncating at k = 10
kMax = 10
post = rep(NA,kMax+1)
utility = rep(NA,kMax+1)
for (k in 0:kMax){
post[k+1] <- gamma(k + sumX + beta_)/gamma(k + sumX + n + alpha_ + beta_ +1)
utility[k+1] <- (2^k-1)-2
}
post <- post/sum(post)
barplot(post, names.arg = 0:kMax, main = "Predictive distribution", xlab = "x_(n+1)", ylab = "Probability")
barplot(post_dens(seq(0,10,1)), type="l")
exp_util=0
posterior_prob=post_dens(seq(1,11))
posterior_prob=posterior_prob/sum(posterior_prob)
for (k in 1:11) {
exp_util=exp_util+(2^k-3)*post_dens(k)
}
# We need to decide an upper bound for k (we can't compute expected values by summing over an infinite set of number)
# Let's start by truncating at k = 10
kMax = 10
post = rep(NA,kMax+1)
utility = rep(NA,kMax+1)
for (k in 0:kMax){
post[k+1] <- gamma(k + sumX + beta_)/gamma(k + sumX + n + alpha_ + beta_ +1)
utility[k+1] <- (2^k-1)-2
}
post <- post/sum(post)
exp_util=0
posterior_prob=post_dens(seq(1,11))
seq(1,11)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Exam/2017-05-30/2017-05-30_Assignment4.R', echo=TRUE)
exp_util=0
posterior_prob=post_dens(seq(1,11))
posterior_prob=posterior_prob/sum(posterior_prob)
for (k in 1:10) {
exp_util=exp_util+(2^k-3)*post_dens(k)
print(exp_util)
}
posterior_prob=post_dens(seq(1,10))
posterior_prob=posterior_prob/sum(posterior_prob)
posterior_prob=post_dens(seq(1,10))
posterior_prob=posterior_prob/sum(posterior_prob)
exp_util=c()
for (k in 1:10) {
exp_util=c(exp_util,(2^k-3))
}
exp_post_dens=sum(posterior_prob*exp_util)
print(exp_post_dens)
posterior_prob=post_dens(seq(1,10))
posterior_prob=posterior_prob/sum(posterior_prob)
exp_util=c()
for (k in 1:10) {
exp_util=c(exp_util,(2^k-3))
}
exp_post_dens=sum(posterior_prob*exp_util)
print(exp_post_dens)
posterior_prob=post_dens(seq(1,11))
posterior_prob=posterior_prob/sum(posterior_prob)
exp_util=c()
for (k in 1:11) {
exp_util=c(exp_util,(2^k-3))
}
exp_post_dens=sum(posterior_prob*exp_util)
print(exp_post_dens)
posterior_prob=post_dens(seq(1,9))
posterior_prob=posterior_prob/sum(posterior_prob)
exp_util=c()
for (k in 1:9) {
exp_util=c(exp_util,(2^k-3))
}
exp_post_dens=sum(posterior_prob*exp_util)
print(exp_post_dens)
posterior_prob=post_dens(seq(0,10))
posterior_prob=posterior_prob/sum(posterior_prob)
exp_util=c()
for (k in 0:10) {
exp_util=c(exp_util,(2^k-3))
}
exp_post_dens=sum(posterior_prob*exp_util)
print(exp_post_dens)
